<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>RNN and NLP Study Notes 02 | Tristan's Blog</title>
<meta name=keywords content><meta name=description content='Text Processing and Word Embedding
Text to Sequence
Processing text data is crucial for natural language processing (NLP) and machine learning applications. This section outlines the key steps in processing text data.
Step 1: Tokenization
The first step in text processing is tokenization, which involves breaking down a text string into a list of individual words. For example, given the text:


1


S = "Machine learning is an important branch of artificial intelligence"  


Breaking this string into a word list:'><meta name=author content="Tristan"><link rel=canonical href=http://localhost:1313/posts/nlp02/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.392aecb5c92792e8888f6bee404f8c562ed37691c3e5e0914aa5cd0a29ec8350.css integrity="sha256-OSrstcknkuiIj2vuQE+MVi7TdpHD5eCRSqXNCinsg1A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/favicon/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/nlp02/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:url" content="http://localhost:1313/posts/nlp02/"><meta property="og:site_name" content="Tristan's Blog"><meta property="og:title" content="RNN and NLP Study Notes 02"><meta property="og:description" content='Text Processing and Word Embedding Text to Sequence Processing text data is crucial for natural language processing (NLP) and machine learning applications. This section outlines the key steps in processing text data.
Step 1: Tokenization The first step in text processing is tokenization, which involves breaking down a text string into a list of individual words. For example, given the text:
1 S = "Machine learning is an important branch of artificial intelligence" Breaking this string into a word list:'><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-24T11:41:20+08:00"><meta property="article:modified_time" content="2025-03-24T11:41:20+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="RNN and NLP Study Notes 02"><meta name=twitter:description content='Text Processing and Word Embedding
Text to Sequence
Processing text data is crucial for natural language processing (NLP) and machine learning applications. This section outlines the key steps in processing text data.
Step 1: Tokenization
The first step in text processing is tokenization, which involves breaking down a text string into a list of individual words. For example, given the text:


1


S = "Machine learning is an important branch of artificial intelligence"  


Breaking this string into a word list:'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"RNN and NLP Study Notes 02","item":"http://localhost:1313/posts/nlp02/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"RNN and NLP Study Notes 02","name":"RNN and NLP Study Notes 02","description":"Text Processing and Word Embedding Text to Sequence Processing text data is crucial for natural language processing (NLP) and machine learning applications. This section outlines the key steps in processing text data.\nStep 1: Tokenization The first step in text processing is tokenization, which involves breaking down a text string into a list of individual words. For example, given the text:\n1 S = \u0026#34;Machine learning is an important branch of artificial intelligence\u0026#34; Breaking this string into a word list:\n","keywords":[""],"articleBody":"Text Processing and Word Embedding Text to Sequence Processing text data is crucial for natural language processing (NLP) and machine learning applications. This section outlines the key steps in processing text data.\nStep 1: Tokenization The first step in text processing is tokenization, which involves breaking down a text string into a list of individual words. For example, given the text:\n1 S = \"Machine learning is an important branch of artificial intelligence\" Breaking this string into a word list:\n1 L = [\"machine\", \"learning\", \"is\", \"an\", \"important\", \"branch\", \"of\", \"artificial\", \"intelligence\"] This shows the simplest form of tokenization, but in reality, many factors need to be considered, such as:\nConverting upper case to lower case (e.g., changing “Apple” to “apple”). Removing stop words, such as “the,” “a,” “of,” etc. Correcting typos (e.g., changing “goood” to “good”). Nowadays, commonly used tokenization methods include BPE (Byte Pair Encoding), WordPiece, and SentencePiece.\nStep 2: Build Dictionary After obtaining the word list, the next step is to build a dictionary that maps each word to a unique index and counts the frequency of each word. This can be accomplished using a dictionary (hash table) that records each word along with its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If $w$ is not in the dictionary, add $w$ and set its frequency to 1. If $w$ is already in the dictionary, increment its frequency counter. After processing some text, the dictionary might look like this:\nWord Frequency machine learning 219 artificial intelligence 200 deep learning 180 model 131 algorithm 120 training 52 prediction 31 Step 3: One-Hot Encoding Using a dictionary to map words to indices\nUtilizing the previously constructed dictionary, map each word in the text to its corresponding index (integer). Then, these indices form a sequence. For example, suppose the dictionary contains the following entries derived from the frequency table:\nWord Index machine learning 1 artificial intelligence 2 deep learning 3 model 4 algorithm 5 training 6 prediction 7 an 8 important 9 branch 10 of 11 intelligence 12 is 13 The corresponding sequence for the sentence would be:\n1 sequences = [1, 13, 8, 9, 10, 11, 2] Generating one-hot vectors\nFor each word in the sequence, a one-hot vector is generated based on its index. A one-hot vector is a $v$-dimensional vector, where $v$ is the size of the vocabulary.\nGiven the vocabulary size of 13 (assuming distinct words from the example), each word’s one-hot vector will be structured as follows:\n1 2 3 4 5 6 7 \"machine learning\" = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \"artificial intelligence\" = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \"is\" = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] \"an\" = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] \"important\" = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] \"branch\" = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] \"of\" = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] For the example sentence “Machine learning is an important branch of artificial intelligence”, the corresponding one-hot encoding results would be:\n1 2 3 4 5 6 7 8 9 [ [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], // \"machine learning\" [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], // \"is\" [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], // \"an\" [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], // \"important\" [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], // \"branch\" [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], // \"of\" [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], // \"artificial intelligence\" ] In this way, all words in the example sentence are transformed into corresponding one-hot encodings, which can be used for subsequent machine learning tasks.\nStep 4: Align Sequences In the process of text processing, Aligning Sequences is a crucial step to ensure that all input samples have the same length. After the previous processing steps, we found that there are differences in the lengths of the sequences, which poses a challenge for building machine learning models. Most machine learning models require uniform input shapes, so we must perform sequence alignment.\nTo address this issue, the following measures can be implemented:\nFixed Length: Choose a fixed length $w$. Truncation: If a sequence exceeds the length $w$, retain only the last $w$ words. Padding: If a sequence is shorter than $w$, pad it with zeros until it reaches the specified length $w$. By doing this, all sequences will be adjusted to the same length, allowing them to be effectively stored in a matrix and facilitating subsequent training of machine learning models. This process will help improve the model’s performance and accuracy.\nWord Embedding: Word to Vector Why map words to vectors? The main reasons for mapping words to vectors in natural language processing (NLP) include:\nCapturing semantic relationships\nTraditional word representations (like one-hot vectors) only indicate the presence or absence of words, lacking the ability to express similarities between words. By mapping to a low-dimensional vector space, word embeddings can capture semantic similarities between words. For example, “king” and “queen” would be closer in vector space, while “king” and “apple” would be farther apart.\nDimensionality reduction\nOne-hot encoding generates a sparse vector with a dimension equal to the size of the vocabulary, leading to significant storage and computational costs. Word embeddings represent words as low-dimensional dense vectors, significantly reducing the required storage space and computation.\nImproving model performance\nUsing low-dimensional embedding vectors allows machine learning models to learn and predict more efficiently. Embedding vectors enable models to better understand and process complex patterns in language, thus enhancing performance in tasks like sentiment analysis and text classification.\nFacilitating transfer learning\nPre-trained word embedding vectors can be transferred between different tasks and datasets, accelerating model training and improving prediction accuracy. This makes word embeddings widely used in various NLP tasks.\nHow to map word to vector? Represent words using one-hot vectors\nFirst, represent words using one-hot vectors.\nAssume the dictionary contains $v$ unique words (vocabulary = $v$). Then the one-hot vectors $e_1$, $e_2$, $e_3$, …, $e_v$ are $v$-dimensional. Map one-hot vectors to low-dimensional vectors\nNext, map the one-hot vectors to low-dimensional vectors. The mapping formula is: $$ x_i = P^T \\cdot e_i $$\n$x_i$ is the low-dimensional vector, with dimensions $d \\times 1$. $P$ is parameter matrix which can be learned from training data, with dimensions $d \\times v$. $e_i$ is the one-hot vector of the 𝑖-th word in dictionary, with dimensions $v \\times 1$ Interpretation of the parameter matrix The parameter matrix $P$ contains the embedding representations of each word in the low-dimensional space. Each row corresponds to a word.\nBy visualizing, you can see the relative positions of different words in low-dimensional space. Similar words are close to each other in vector space, reflecting their semantic similarity.\nFor example, the position of the word “fantastic” is close to “good,” “fun,” etc., while “boring” and “poor” are relatively far apart.\nConclusion Text processing converts raw text into structured formats suitable for analysis and consists of several key steps:\nTokenization: Breaking down text into individual words. Building a Dictionary: Mapping each word to a unique index and counting frequencies. One-Hot Encoding: Converting words to one-hot vectors. Aligning Sequences: Ensuring uniform input lengths for machine learning models. Mastering these steps is essential for efficient feature extraction and improved model performance in natural language processing tasks.\nWord embeddings transform words into low-dimensional vectors, capturing semantic relationships and enhancing expressiveness for various natural language processing tasks. The process involves:\nOne-Hot Encoding: Representing words as sparse vectors. Mapping to Low-Dimensional Vectors: Using a parameter matrix to project one-hot vectors into a reduced space. Understanding how to map words to vectors and interpret the parameter matrix is crucial for developing effective NLP applications.\nReferences For a deeper understanding of these concepts, you can refer to the following course video:RNN模型与NLP应用(2/9)：文本处理与词嵌入.\n","wordCount":"1377","inLanguage":"en","datePublished":"2025-03-24T11:41:20+08:00","dateModified":"2025-03-24T11:41:20+08:00","author":{"@type":"Person","name":"Tristan"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/nlp02/"},"publisher":{"@type":"Organization","name":"Tristan's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Tristan's Blog (Alt + H)">Tristan's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/resume/ title=Resume><span>Resume</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">RNN and NLP Study Notes 02</h1><div class=post-meta><span title='2025-03-24 11:41:20 +0800 HKT'>March 24, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1377 words&nbsp;·&nbsp;Tristan&nbsp;|&nbsp;<a href=https://github.com/BaizeXS/baizexs.github.io/tree/main/content/posts/NLP02.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#text-processing-and-word-embedding>Text Processing and Word Embedding</a><ul><li><a href=#text-to-sequence>Text to Sequence</a></li><li><a href=#word-embedding-word-to-vector>Word Embedding: Word to Vector</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><h2 id=text-processing-and-word-embedding>Text Processing and Word Embedding<a hidden class=anchor aria-hidden=true href=#text-processing-and-word-embedding>#</a></h2><h3 id=text-to-sequence>Text to Sequence<a hidden class=anchor aria-hidden=true href=#text-to-sequence>#</a></h3><p>Processing text data is crucial for natural language processing (NLP) and machine learning applications. This section outlines the key steps in processing text data.</p><h4 id=step-1-tokenization>Step 1: Tokenization<a hidden class=anchor aria-hidden=true href=#step-1-tokenization>#</a></h4><p>The first step in text processing is <strong>tokenization</strong>, which involves <strong>breaking down a text string into a list of individual words</strong>. For example, given the text:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>S = &#34;Machine learning is an important branch of artificial intelligence&#34;  
</span></span></code></pre></td></tr></table></div></div><p>Breaking this string into a word list:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>L = [&#34;machine&#34;, &#34;learning&#34;, &#34;is&#34;, &#34;an&#34;, &#34;important&#34;, &#34;branch&#34;, &#34;of&#34;, &#34;artificial&#34;, &#34;intelligence&#34;]
</span></span></code></pre></td></tr></table></div></div><p>This shows the simplest form of tokenization, but in reality, many factors need to be considered, such as:</p><ul><li>Converting upper case to lower case (e.g., changing &ldquo;Apple&rdquo; to &ldquo;apple&rdquo;).</li><li>Removing stop words, such as &ldquo;the,&rdquo; &ldquo;a,&rdquo; &ldquo;of,&rdquo; etc.</li><li>Correcting typos (e.g., changing &ldquo;goood&rdquo; to &ldquo;good&rdquo;).</li></ul><p>Nowadays, commonly used tokenization methods include <strong>BPE (Byte Pair Encoding)</strong>, <strong>WordPiece</strong>, and <strong>SentencePiece</strong>.</p><h4 id=step-2-build-dictionary>Step 2: Build Dictionary<a hidden class=anchor aria-hidden=true href=#step-2-build-dictionary>#</a></h4><p>After obtaining the word list, the next step is to <strong>build a dictionary that maps each word to a unique index</strong> and <strong>counts the frequency of each word</strong>. This can be accomplished <strong>using a dictionary (hash table)</strong> that records each word along with its corresponding frequency.</p><ul><li>Initially, the dictionary is empty.</li><li>For each word ($w$):<ul><li>If $w$ is not in the dictionary, add $w$ and set its frequency to 1.</li><li>If $w$ is already in the dictionary, increment its frequency counter.</li></ul></li></ul><p>After processing some text, the dictionary might look like this:</p><table><thead><tr><th>Word</th><th>Frequency</th></tr></thead><tbody><tr><td>machine learning</td><td>219</td></tr><tr><td>artificial intelligence</td><td>200</td></tr><tr><td>deep learning</td><td>180</td></tr><tr><td>model</td><td>131</td></tr><tr><td>algorithm</td><td>120</td></tr><tr><td>training</td><td>52</td></tr><tr><td>prediction</td><td>31</td></tr></tbody></table><h4 id=step-3-one-hot-encoding>Step 3: One-Hot Encoding<a hidden class=anchor aria-hidden=true href=#step-3-one-hot-encoding>#</a></h4><p><strong>Using a dictionary to map words to indices</strong></p><p>Utilizing the previously constructed dictionary, map each word in the text to its corresponding index (integer). Then, these indices form a sequence. For example, suppose the dictionary contains the following entries derived from the frequency table:</p><table><thead><tr><th>Word</th><th>Index</th></tr></thead><tbody><tr><td>machine learning</td><td>1</td></tr><tr><td>artificial intelligence</td><td>2</td></tr><tr><td>deep learning</td><td>3</td></tr><tr><td>model</td><td>4</td></tr><tr><td>algorithm</td><td>5</td></tr><tr><td>training</td><td>6</td></tr><tr><td>prediction</td><td>7</td></tr><tr><td>an</td><td>8</td></tr><tr><td>important</td><td>9</td></tr><tr><td>branch</td><td>10</td></tr><tr><td>of</td><td>11</td></tr><tr><td>intelligence</td><td>12</td></tr><tr><td>is</td><td>13</td></tr></tbody></table><p>The corresponding sequence for the sentence would be:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>sequences = [1, 13, 8, 9, 10, 11, 2]
</span></span></code></pre></td></tr></table></div></div><p><strong>Generating one-hot vectors</strong></p><p>For each word in the sequence, a one-hot vector is generated based on its index. A one-hot vector is a $v$-dimensional vector, where $v$ is the size of the vocabulary.</p><p>Given the vocabulary size of 13 (assuming distinct words from the example), each word&rsquo;s one-hot vector will be structured as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&#34;machine learning&#34; = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</span></span><span class=line><span class=cl>&#34;artificial intelligence&#34; = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</span></span><span class=line><span class=cl>&#34;is&#34; = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
</span></span><span class=line><span class=cl>&#34;an&#34; = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
</span></span><span class=line><span class=cl>&#34;important&#34; = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
</span></span><span class=line><span class=cl>&#34;branch&#34; = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
</span></span><span class=line><span class=cl>&#34;of&#34; = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
</span></span></code></pre></td></tr></table></div></div><p>For the example sentence &ldquo;Machine learning is an important branch of artificial intelligence&rdquo;, the corresponding one-hot encoding results would be:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>[  
</span></span><span class=line><span class=cl>  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  // &#34;machine learning&#34;  
</span></span><span class=line><span class=cl>  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],  // &#34;is&#34;  
</span></span><span class=line><span class=cl>  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],  // &#34;an&#34;  
</span></span><span class=line><span class=cl>  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],  // &#34;important&#34;  
</span></span><span class=line><span class=cl>  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],  // &#34;branch&#34;  
</span></span><span class=line><span class=cl>  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],  // &#34;of&#34;  
</span></span><span class=line><span class=cl>  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  // &#34;artificial intelligence&#34;
</span></span><span class=line><span class=cl>]  
</span></span></code></pre></td></tr></table></div></div><p>In this way, all words in the example sentence are transformed into corresponding one-hot encodings, which can be used for subsequent machine learning tasks.</p><h4 id=step-4-align-sequences>Step 4: Align Sequences<a hidden class=anchor aria-hidden=true href=#step-4-align-sequences>#</a></h4><p>In the process of text processing, Aligning Sequences is a crucial step to ensure that <strong>all input samples have the same length</strong>. After the previous processing steps, we found that there are differences in the lengths of the sequences, which poses a challenge for building machine learning models. Most machine learning models require uniform input shapes, so we must perform sequence alignment.</p><p>To address this issue, the following measures can be implemented:</p><ul><li><strong>Fixed Length</strong>: Choose a fixed length $w$.</li><li><strong>Truncation</strong>: If a sequence exceeds the length $w$, retain only the last $w$ words.</li><li><strong>Padding</strong>: If a sequence is shorter than $w$, pad it with zeros until it reaches the specified length $w$.</li></ul><p><img alt=image-20250325141534011 loading=lazy src=/images/image-20250325141534011.png></p><p>By doing this, all sequences will be adjusted to the same length, allowing them to be effectively stored in a matrix and facilitating subsequent training of machine learning models. This process will help improve the model&rsquo;s performance and accuracy.</p><p><img alt=image-20250325141950386 loading=lazy src=/images/image-20250325141950386.png></p><h3 id=word-embedding-word-to-vector>Word Embedding: Word to Vector<a hidden class=anchor aria-hidden=true href=#word-embedding-word-to-vector>#</a></h3><h4 id=why-map-words-to-vectors>Why map words to vectors?<a hidden class=anchor aria-hidden=true href=#why-map-words-to-vectors>#</a></h4><p>The main reasons for mapping words to vectors in natural language processing (NLP) include:</p><ol><li><p><strong>Capturing semantic relationships</strong></p><p>Traditional word representations (like one-hot vectors) only indicate the presence or absence of words, lacking the ability to express similarities between words. By mapping to a low-dimensional vector space, word embeddings can capture semantic similarities between words. For example, &ldquo;king&rdquo; and &ldquo;queen&rdquo; would be closer in vector space, while &ldquo;king&rdquo; and &ldquo;apple&rdquo; would be farther apart.</p></li><li><p><strong>Dimensionality reduction</strong></p><p>One-hot encoding generates a sparse vector with a dimension equal to the size of the vocabulary, leading to significant storage and computational costs. Word embeddings represent words as low-dimensional dense vectors, significantly reducing the required storage space and computation.</p></li><li><p><strong>Improving model performance</strong></p><p>Using low-dimensional embedding vectors allows machine learning models to learn and predict more efficiently. Embedding vectors enable models to better understand and process complex patterns in language, thus enhancing performance in tasks like sentiment analysis and text classification.</p></li><li><p><strong>Facilitating transfer learning</strong></p><p>Pre-trained word embedding vectors can be transferred between different tasks and datasets, accelerating model training and improving prediction accuracy. This makes word embeddings widely used in various NLP tasks.</p></li></ol><h4 id=how-to-map-word-to-vector>How to map word to vector?<a hidden class=anchor aria-hidden=true href=#how-to-map-word-to-vector>#</a></h4><ol><li><p><strong>Represent words using one-hot vectors</strong></p><p>First, represent words using one-hot vectors.</p><ul><li>Assume the dictionary contains $v$ unique words (vocabulary = $v$).</li><li>Then the one-hot vectors $e_1$, $e_2$, $e_3$, …, $e_v$ are $v$-dimensional.</li></ul></li><li><p><strong>Map one-hot vectors to low-dimensional vectors</strong></p><p>Next, map the one-hot vectors to low-dimensional vectors. The mapping formula is:
$$
x_i = P^T \cdot e_i
$$</p><ul><li>$x_i$ is the low-dimensional vector, with dimensions $d \times 1$.</li><li>$P$ is parameter matrix which can be learned from training data, with dimensions $d \times v$.</li><li>$e_i$ is the one-hot vector of the 𝑖-th word in dictionary, with dimensions $v \times 1$</li></ul><figure><img loading=lazy src=/images/image-20250325143845492.png alt=image-20250325143845492></figure></li></ol><h4 id=interpretation-of-the-parameter-matrix>Interpretation of the parameter matrix<a hidden class=anchor aria-hidden=true href=#interpretation-of-the-parameter-matrix>#</a></h4><ul><li><p>The parameter matrix $P$ contains the embedding representations of each word in the low-dimensional space. <strong>Each row corresponds to a word</strong>.</p></li><li><p>By visualizing, you can see the relative positions of different words in low-dimensional space. <strong>Similar words are close to each other in vector space</strong>, reflecting their semantic similarity.</p><p>For example, the position of the word &ldquo;fantastic&rdquo; is close to &ldquo;good,&rdquo; &ldquo;fun,&rdquo; etc., while &ldquo;boring&rdquo; and &ldquo;poor&rdquo; are relatively far apart.</p><figure><img loading=lazy src=/images/image-20250325143922568.png alt=image-20250325143922568></figure></li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Text processing converts raw text into structured formats suitable for analysis and consists of several key steps:</p><ol><li><strong>Tokenization</strong>: Breaking down text into individual words.</li><li><strong>Building a Dictionary</strong>: Mapping each word to a unique index and counting frequencies.</li><li><strong>One-Hot Encoding</strong>: Converting words to one-hot vectors.</li><li><strong>Aligning Sequences</strong>: Ensuring uniform input lengths for machine learning models.</li></ol><p>Mastering these steps is essential for efficient feature extraction and improved model performance in natural language processing tasks.</p><p>Word embeddings transform words into low-dimensional vectors, capturing semantic relationships and enhancing expressiveness for various natural language processing tasks. The process involves:</p><ol><li><strong>One-Hot Encoding</strong>: Representing words as sparse vectors.</li><li><strong>Mapping to Low-Dimensional Vectors</strong>: Using a parameter matrix to project one-hot vectors into a reduced space.</li></ol><p>Understanding how to map words to vectors and interpret the parameter matrix is crucial for developing effective NLP applications.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>For a deeper understanding of these concepts, you can refer to the following course video:<a href="https://youtu.be/6_2_2CPB97s?si=dUyiiUyIeKITKTxN">RNN模型与NLP应用(2/9)：文本处理与词嵌入</a>.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/nlp03/><span class=title>« Prev</span><br><span>RNN and NLP Study Notes 03</span>
</a><a class=next href=http://localhost:1313/posts/nlp01/><span class=title>Next »</span><br><span>RNN and NLP Study Notes 01</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 02 on x" href="https://x.com/intent/tweet/?text=RNN%20and%20NLP%20Study%20Notes%2002&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp02%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 02 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp02%2f&amp;title=RNN%20and%20NLP%20Study%20Notes%2002&amp;summary=RNN%20and%20NLP%20Study%20Notes%2002&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp02%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 02 on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp02%2f&title=RNN%20and%20NLP%20Study%20Notes%2002"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 02 on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp02%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 02 on whatsapp" href="https://api.whatsapp.com/send?text=RNN%20and%20NLP%20Study%20Notes%2002%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp02%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 02 on telegram" href="https://telegram.me/share/url?text=RNN%20and%20NLP%20Study%20Notes%2002&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp02%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 02 on ycombinator" href="https://news.ycombinator.com/submitlink?t=RNN%20and%20NLP%20Study%20Notes%2002&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp02%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Tristan's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>