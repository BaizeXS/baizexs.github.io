<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>RNN and NLP Study Notes 03 | Tristan's Blog</title>
<meta name=keywords content><meta name=description content="Recurrent Neural Networks (RNNs)
How to model sequential data?
In deep learning, the key to processing sequential data lies in understanding the relationship between inputs and outputs. Generally, the input-output relationships of data can be divided into three categories: one-to-one, one-to-many, and many-to-many. A one-to-one relationship means that each input has a corresponding output, which is commonly used in image classification. In contrast, a one-to-many relationship indicates that a single input can produce multiple outputs, suitable for generation tasks, such as a chatbot generating multiple responses based on user input. Meanwhile, a many-to-many relationship involves multiple inputs leading to multiple outputs, typically used in sequence transformation tasks, such as machine translation."><meta name=author content="Tristan"><link rel=canonical href=http://localhost:1313/posts/nlp03/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.392aecb5c92792e8888f6bee404f8c562ed37691c3e5e0914aa5cd0a29ec8350.css integrity="sha256-OSrstcknkuiIj2vuQE+MVi7TdpHD5eCRSqXNCinsg1A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/favicon/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/nlp03/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:url" content="http://localhost:1313/posts/nlp03/"><meta property="og:site_name" content="Tristan's Blog"><meta property="og:title" content="RNN and NLP Study Notes 03"><meta property="og:description" content="Recurrent Neural Networks (RNNs) How to model sequential data? In deep learning, the key to processing sequential data lies in understanding the relationship between inputs and outputs. Generally, the input-output relationships of data can be divided into three categories: one-to-one, one-to-many, and many-to-many. A one-to-one relationship means that each input has a corresponding output, which is commonly used in image classification. In contrast, a one-to-many relationship indicates that a single input can produce multiple outputs, suitable for generation tasks, such as a chatbot generating multiple responses based on user input. Meanwhile, a many-to-many relationship involves multiple inputs leading to multiple outputs, typically used in sequence transformation tasks, such as machine translation."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-24T11:41:32+08:00"><meta property="article:modified_time" content="2025-03-24T11:41:32+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="RNN and NLP Study Notes 03"><meta name=twitter:description content="Recurrent Neural Networks (RNNs)
How to model sequential data?
In deep learning, the key to processing sequential data lies in understanding the relationship between inputs and outputs. Generally, the input-output relationships of data can be divided into three categories: one-to-one, one-to-many, and many-to-many. A one-to-one relationship means that each input has a corresponding output, which is commonly used in image classification. In contrast, a one-to-many relationship indicates that a single input can produce multiple outputs, suitable for generation tasks, such as a chatbot generating multiple responses based on user input. Meanwhile, a many-to-many relationship involves multiple inputs leading to multiple outputs, typically used in sequence transformation tasks, such as machine translation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"RNN and NLP Study Notes 03","item":"http://localhost:1313/posts/nlp03/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"RNN and NLP Study Notes 03","name":"RNN and NLP Study Notes 03","description":"Recurrent Neural Networks (RNNs) How to model sequential data? In deep learning, the key to processing sequential data lies in understanding the relationship between inputs and outputs. Generally, the input-output relationships of data can be divided into three categories: one-to-one, one-to-many, and many-to-many. A one-to-one relationship means that each input has a corresponding output, which is commonly used in image classification. In contrast, a one-to-many relationship indicates that a single input can produce multiple outputs, suitable for generation tasks, such as a chatbot generating multiple responses based on user input. Meanwhile, a many-to-many relationship involves multiple inputs leading to multiple outputs, typically used in sequence transformation tasks, such as machine translation.\n","keywords":[""],"articleBody":"Recurrent Neural Networks (RNNs) How to model sequential data? In deep learning, the key to processing sequential data lies in understanding the relationship between inputs and outputs. Generally, the input-output relationships of data can be divided into three categories: one-to-one, one-to-many, and many-to-many. A one-to-one relationship means that each input has a corresponding output, which is commonly used in image classification. In contrast, a one-to-many relationship indicates that a single input can produce multiple outputs, suitable for generation tasks, such as a chatbot generating multiple responses based on user input. Meanwhile, a many-to-many relationship involves multiple inputs leading to multiple outputs, typically used in sequence transformation tasks, such as machine translation.\nOne to One: Definition: Each input corresponds to one output. Application Example: Commonly used in image classification tasks, where the input is an image and the output is the label of that image. One to Many: Definition: One input can generate multiple outputs. Application Example: In a chatbot, multiple possible responses can be generated after inputting a question from the user, or multiple descriptions can be generated for an image. Many to Many: Definition: Multiple inputs can produce multiple outputs, and the lengths of the sequences are usually different. Application Example: In machine translation tasks, multiple words from the input sentence correspond to multiple words in the translated output. Fully connected networks (FC Nets) and convolutional neural networks (ConvNets) perform well when handling fixed-size inputs and outputs, but they struggle with variable-length sequential data. Since these two models typically only allow for one-to-one or one-to-many mappings, they cannot capture the temporal dependencies and contextual information between inputs.\nLimitations of FC Nets and ConvNets:\nProcess a paragraph as a whole. Fixed-size input (e.g., image). Fixed-size output (e.g., predicted probabilities). In comparison, recurrent neural networks (RNNs) are more suitable for modeling sequential data. RNNs can handle variable-length inputs and outputs while continuously preserving contextual information through hidden states, which enables them to demonstrate greater flexibility and capability in tasks such as sequential generation and time series analysis. RNNs are better ways to model sequential data (e.g., text, speech, and time series).\nRecurrent Neural Networks (RNNs) are designed to handle sequential data, where the relationships between inputs and outputs are often many-to-one or many-to-many rather than one-to-one. In an RNN, each word in a sequence contributes to the accumulation of information in the state vector $h_t$. Initially, each input word is transformed into a word embedding, producing a corresponding vector $x_t$. As each word vector is fed into the RNN, the network updates the state $h$ to incorporate the new information. For instance, $h_0$ encapsulates the information from the first word $x_0$, while $h_1$ contains information from the first two words $x_0$ and $x_1$, and so on. By the time the last word is processed, the final state $h_t$ represents a feature vector that summarizes the entire sentence. The state updates rely on a parameter matrix $A$, which remains constant throughout the RNN, regardless of the length of the sequence being processed. This design allows RNNs to effectively capture dependencies across sequences and utilize contextual information.\nSimple RNN Model Recurrent Neural Networks (RNN) Models and Working Principles Recurrent Neural Networks (RNNs) are a type of neural network model specifically designed for handling sequential data. Their structure allows the state at each moment to depend not only on the current input but also on the previous state, creating a dynamic memory mechanism. In an RNN, the two inputs to the model are the previous state $h_{t-1}$ and the current input word vector $x_t$.\nDuring the state update, the RNN first concatenates $h_{t-1}$ and $x_t$ to generate a higher-dimensional vector. This vector is then multiplied by the model’s parameter matrix $A$ to obtain a new vector. This new vector is processed through an activation function, which is typically the hyperbolic tangent function $\\tanh$, serving to compress each element’s value within the range of -1 to 1. In this way, the state $h_t$ is effectively updated, allowing us to interpret the new state $h_t$ as a function of the current input $x_t$ and the previous state $h_{t-1}$.\nThe computation of state $h_t$ relies on three factors:\nthe current input $x_t$, the previous state $h_{t-1}$, and the parameter matrix $A$. The parameter matrix $A$ plays a crucial role in the calculations of the entire RNN, driving the process of each state update.\nChoosing the Activation Function: Why Use the Tanh Function? The activation function plays a key role in deep learning models. Without an appropriate activation function, the model may encounter issues of gradient vanishing or explosion. The function of tanh is to perform “normalization” after each weight update. By readjusting the values to a reasonable range of -1 to 1, the tanh function helps maintain the stability of the model and enhances training efficiency. This normalization process ensures that the model can learn and update the weights of each layer more accurately, thereby improving overall performance.\nModel Parameters of Simple RNN When discussing the parameters of RNNs, attention should be paid to the dimensions of the parameter matrix $A$. Specifically, the number of rows in matrix $A$ corresponds to the dimension of the hidden state $shape(h)$, while the number of columns is the sum of the hidden state dimension and the input dimension ($shape(h) + shape(x)$). Therefore, the total number of parameters in this matrix can be represented as:\n#rows of $A$: $shape(h)$ #cols of $A$: $shape(h) + shape(x)$ Total #parameter: $shape(h) \\times [shape(h) + shape(x)]$. This means that as the input scale increases, the number of parameters will also grow, thereby influencing the model’s complexity and training requirements.\nLimitations of Simple RNN Although Simple RNNs perform well in handling short-term dependencies, they exhibit significant shortcomings in addressing long-term dependencies. This is because, in RNNs, the current state $h$ is functionally related to all previous states $h$. Theoretically, if the early input $x_1$ is changed, it should lead to changes in all subsequent states $h$. However, in practice, Simple RNNs do not fully demonstrate this property. By taking the derivative of state $h_{100}$ with respect to $x_1$, it can be observed that the derivative approaches zero, indicating that when $x_1$ changes, $h_{100}$ hardly changes at all. This shows that state $h_{100}$ has almost no relation to earlier inputs $x_0$, meaning that state $h_{100}$ has forgotten information from many previous steps, highlighting the limitations of RNNs in handling long-term dependency issues.\nThese designs and limitations point to the use cases of Simple RNNs as well as the directions for future model improvements, leading to the development of more complex RNN variants such as Long Short-Term Memory networks (LSTMs) and Gated Recurrent Units (GRUs), aimed at addressing issues of long-term dependencies.\nSummary RNN for text, speech, and time series data.\nHidden state $h_t$ aggregates information in the inputs $x_0, \\dots, x_t$.\nRNNs can forget early inputs.\nIt forgets what it has seen early on. If $t$ is large, $h_t$ is almost irrelevant to $x_0$. SimpleRNN has a parameter matrix (and perhaps an intercept vector).\nShape of the parameter matrix is\n$$ shape(h) \\times [shape(h) + shape(x)]. $$\nOnly one such parameter matrix, no matter how long the sequence is.\n","wordCount":"1192","inLanguage":"en","datePublished":"2025-03-24T11:41:32+08:00","dateModified":"2025-03-24T11:41:32+08:00","author":{"@type":"Person","name":"Tristan"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/nlp03/"},"publisher":{"@type":"Organization","name":"Tristan's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Tristan's Blog (Alt + H)">Tristan's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/resume/ title=Resume><span>Resume</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">RNN and NLP Study Notes 03</h1><div class=post-meta><span title='2025-03-24 11:41:32 +0800 HKT'>March 24, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1192 words&nbsp;·&nbsp;Tristan&nbsp;|&nbsp;<a href=https://github.com/BaizeXS/baizexs.github.io/tree/main/content/posts/NLP03.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#recurrent-neural-networks-rnns>Recurrent Neural Networks (RNNs)</a><ul><li><a href=#how-to-model-sequential-data>How to model sequential data?</a></li><li><a href=#simple-rnn-model>Simple RNN Model</a></li><li><a href=#summary>Summary</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=recurrent-neural-networks-rnns>Recurrent Neural Networks (RNNs)<a hidden class=anchor aria-hidden=true href=#recurrent-neural-networks-rnns>#</a></h2><h3 id=how-to-model-sequential-data>How to model sequential data?<a hidden class=anchor aria-hidden=true href=#how-to-model-sequential-data>#</a></h3><p>In deep learning, the key to processing sequential data lies in <strong>understanding the relationship between inputs and outputs</strong>. Generally, the input-output relationships of data can be divided into three categories: <strong>one-to-one</strong>, <strong>one-to-many</strong>, and <strong>many-to-many</strong>. A one-to-one relationship means that each input has a corresponding output, which is commonly used in image classification. In contrast, a one-to-many relationship indicates that a single input can produce multiple outputs, suitable for generation tasks, such as a chatbot generating multiple responses based on user input. Meanwhile, a many-to-many relationship involves multiple inputs leading to multiple outputs, typically used in sequence transformation tasks, such as machine translation.</p><ul><li><strong>One to One</strong>:<ul><li><strong>Definition</strong>: Each input corresponds to one output.</li><li><strong>Application Example</strong>: Commonly used in image classification tasks, where the input is an image and the output is the label of that image.</li></ul></li><li><strong>One to Many</strong>:<ul><li><strong>Definition</strong>: One input can generate multiple outputs.</li><li><strong>Application Example</strong>: In a chatbot, multiple possible responses can be generated after inputting a question from the user, or multiple descriptions can be generated for an image.</li></ul></li><li><strong>Many to Many</strong>:<ul><li><strong>Definition</strong>: Multiple inputs can produce multiple outputs, and the lengths of the sequences are usually different.</li><li><strong>Application Example</strong>: In machine translation tasks, multiple words from the input sentence correspond to multiple words in the translated output.</li></ul></li></ul><p>Fully connected networks (FC Nets) and convolutional neural networks (ConvNets) perform well when handling fixed-size inputs and outputs, but they struggle with variable-length sequential data. Since these two models typically only allow for one-to-one or one-to-many mappings, they cannot capture the temporal dependencies and contextual information between inputs.</p><p><strong>Limitations of FC Nets and ConvNets</strong>:</p><ul><li>Process a paragraph as a whole.</li><li>Fixed-size input (e.g., image).</li><li>Fixed-size output (e.g., predicted probabilities).</li></ul><p>In comparison, recurrent neural networks (RNNs) are more suitable for modeling sequential data. RNNs can handle variable-length inputs and outputs while continuously preserving contextual information through hidden states, which enables them to demonstrate greater flexibility and capability in tasks such as sequential generation and time series analysis. RNNs are better ways to model sequential data (e.g., text, speech, and time series).</p><figure><img loading=lazy src=/images/image-20250326104312899.png alt=image-20250326104312899></figure><p>Recurrent Neural Networks (RNNs) are designed to handle sequential data, where the relationships between inputs and outputs are often many-to-one or many-to-many rather than one-to-one. In an RNN, each word in a sequence contributes to the accumulation of information in the state vector $h_t$. Initially, each input word is transformed into a word embedding, producing a corresponding vector $x_t$. As each word vector is fed into the RNN, the network updates the state $h$ to incorporate the new information. For instance, $h_0$ encapsulates the information from the first word $x_0$, while $h_1$ contains information from the first two words $x_0$ and $x_1$, and so on. <strong>By the time the last word is processed, the final state $h_t$ represents a feature vector that summarizes the entire sentence.</strong> The state updates rely on a parameter matrix $A$, which remains constant throughout the RNN, regardless of the length of the sequence being processed. This design allows RNNs to effectively capture dependencies across sequences and utilize contextual information.</p><h3 id=simple-rnn-model>Simple RNN Model<a hidden class=anchor aria-hidden=true href=#simple-rnn-model>#</a></h3><h4 id=recurrent-neural-networks-rnn-models-and-working-principles>Recurrent Neural Networks (RNN) Models and Working Principles<a hidden class=anchor aria-hidden=true href=#recurrent-neural-networks-rnn-models-and-working-principles>#</a></h4><p>Recurrent Neural Networks (RNNs) are a type of neural network model specifically designed for handling sequential data. <strong>Their structure allows the state at each moment to depend not only on the current input but also on the previous state, creating a dynamic memory mechanism.</strong> In an RNN, the two inputs to the model are the previous state $h_{t-1}$ and the current input word vector $x_t$.</p><figure><img loading=lazy src=/images/image-20250326105711767.png alt=image-20250326105711767></figure><p>During the state update, the RNN first concatenates $h_{t-1}$ and $x_t$ to generate a higher-dimensional vector. This vector is then multiplied by the model&rsquo;s parameter matrix $A$ to obtain a new vector. This new vector is processed through an activation function, which is typically the hyperbolic tangent function $\tanh$, serving to compress each element&rsquo;s value within the range of -1 to 1. In this way, the state $h_t$ is effectively updated, allowing us to interpret the new state $h_t$ as a function of the current input $x_t$ and the previous state $h_{t-1}$.</p><p>The computation of state $h_t$ relies on three factors:</p><ul><li>the current input $x_t$,</li><li>the previous state $h_{t-1}$,</li><li>and the parameter matrix $A$.</li></ul><p>The parameter matrix $A$ plays a crucial role in the calculations of the entire RNN, driving the process of each state update.</p><h4 id=choosing-the-activation-function-why-use-the-tanh-function>Choosing the Activation Function: Why Use the Tanh Function?<a hidden class=anchor aria-hidden=true href=#choosing-the-activation-function-why-use-the-tanh-function>#</a></h4><p>The activation function plays a key role in deep learning models. Without an appropriate activation function, the model may encounter issues of <strong>gradient vanishing</strong> or <strong>explosion</strong>. The function of tanh is to perform &ldquo;normalization&rdquo; after each weight update. By readjusting the values to a reasonable range of -1 to 1, the tanh function helps <strong>maintain the stability of the model</strong> and <strong>enhances training efficiency</strong>. This normalization process ensures that the model can learn and update the weights of each layer more accurately, thereby improving overall performance.</p><h4 id=model-parameters-of-simple-rnn>Model Parameters of Simple RNN<a hidden class=anchor aria-hidden=true href=#model-parameters-of-simple-rnn>#</a></h4><p>When discussing the parameters of RNNs, attention should be paid to the dimensions of the parameter matrix $A$. Specifically, the number of rows in matrix $A$ corresponds to the dimension of the hidden state $shape(h)$, while the number of columns is the sum of the hidden state dimension and the input dimension ($shape(h) + shape(x)$). Therefore, the total number of parameters in this matrix can be represented as:</p><ul><li>#rows of $A$: $shape(h)$</li><li>#cols of $A$: $shape(h) + shape(x)$</li><li>Total #parameter: $shape(h) \times [shape(h) + shape(x)]$.</li></ul><figure><img loading=lazy src=/images/image-20250326112207607.png alt=image-20250326112207607></figure><p>This means that as the input scale increases, the number of parameters will also grow, thereby influencing the model&rsquo;s complexity and training requirements.</p><h4 id=limitations-of-simple-rnn>Limitations of Simple RNN<a hidden class=anchor aria-hidden=true href=#limitations-of-simple-rnn>#</a></h4><p>Although Simple RNNs perform well in handling short-term dependencies, they exhibit significant <strong>shortcomings in addressing long-term dependencies</strong>. This is because, in RNNs, the current state $h$ is functionally related to all previous states $h$. Theoretically, if the early input $x_1$ is changed, it should lead to changes in all subsequent states $h$. However, in practice, Simple RNNs do not fully demonstrate this property. By taking the derivative of state $h_{100}$ with respect to $x_1$, it can be observed that the derivative approaches zero, indicating that when $x_1$ changes, $h_{100}$ hardly changes at all. This shows that state $h_{100}$ has almost no relation to earlier inputs $x_0$, meaning that state $h_{100}$ has forgotten information from many previous steps, highlighting the limitations of RNNs in handling long-term dependency issues.</p><p>These designs and limitations point to the use cases of Simple RNNs as well as the directions for future model improvements, leading to the development of more complex RNN variants such as Long Short-Term Memory networks (LSTMs) and Gated Recurrent Units (GRUs), aimed at addressing issues of long-term dependencies.</p><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><ul><li><p>RNN for text, speech, and time series data.</p></li><li><p>Hidden state $h_t$ aggregates information in the inputs $x_0, \dots, x_t$.</p></li><li><p>RNNs can forget early inputs.</p><ul><li>It forgets what it has seen early on.</li><li>If $t$ is large, $h_t$ is almost irrelevant to $x_0$.</li></ul></li><li><p>SimpleRNN has a parameter matrix (and perhaps an intercept vector).</p></li><li><p>Shape of the parameter matrix is</p><p>$$
shape(h) \times [shape(h) + shape(x)].
$$</p></li><li><p>Only one such parameter matrix, no matter how long the sequence is.</p></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/nlp04/><span class=title>« Prev</span><br><span>RNN and NLP Study Notes 04</span>
</a><a class=next href=http://localhost:1313/posts/nlp02/><span class=title>Next »</span><br><span>RNN and NLP Study Notes 02</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 03 on x" href="https://x.com/intent/tweet/?text=RNN%20and%20NLP%20Study%20Notes%2003&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp03%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 03 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp03%2f&amp;title=RNN%20and%20NLP%20Study%20Notes%2003&amp;summary=RNN%20and%20NLP%20Study%20Notes%2003&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp03%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 03 on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp03%2f&title=RNN%20and%20NLP%20Study%20Notes%2003"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 03 on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp03%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 03 on whatsapp" href="https://api.whatsapp.com/send?text=RNN%20and%20NLP%20Study%20Notes%2003%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp03%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 03 on telegram" href="https://telegram.me/share/url?text=RNN%20and%20NLP%20Study%20Notes%2003&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp03%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 03 on ycombinator" href="https://news.ycombinator.com/submitlink?t=RNN%20and%20NLP%20Study%20Notes%2003&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp03%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Tristan's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>