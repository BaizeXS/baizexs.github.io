<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>RNN and NLP Study Notes 04 | Tristan's Blog</title>
<meta name=keywords content><meta name=description content="Long Short Term Memory (LSTM)
LSTM Model
Long Short-Term Memory networks (LSTM) are a specialized type of recurrent neural network (RNN) designed to effectively handle dependencies in long sequence data. Traditional RNNs often face the problem of forgetting when processing long time sequences, making it difficult to retain important information. LSTM addresses this issue by introducing a &ldquo;cell state&rdquo; mechanism that allows information to flow directly across time steps, helping the network better capture long-term dependencies."><meta name=author content="Tristan"><link rel=canonical href=http://localhost:1313/posts/nlp04/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.392aecb5c92792e8888f6bee404f8c562ed37691c3e5e0914aa5cd0a29ec8350.css integrity="sha256-OSrstcknkuiIj2vuQE+MVi7TdpHD5eCRSqXNCinsg1A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/favicon/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/nlp04/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:url" content="http://localhost:1313/posts/nlp04/"><meta property="og:site_name" content="Tristan's Blog"><meta property="og:title" content="RNN and NLP Study Notes 04"><meta property="og:description" content="Long Short Term Memory (LSTM) LSTM Model Long Short-Term Memory networks (LSTM) are a specialized type of recurrent neural network (RNN) designed to effectively handle dependencies in long sequence data. Traditional RNNs often face the problem of forgetting when processing long time sequences, making it difficult to retain important information. LSTM addresses this issue by introducing a “cell state” mechanism that allows information to flow directly across time steps, helping the network better capture long-term dependencies."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-24T11:41:35+08:00"><meta property="article:modified_time" content="2025-03-24T11:41:35+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="RNN and NLP Study Notes 04"><meta name=twitter:description content="Long Short Term Memory (LSTM)
LSTM Model
Long Short-Term Memory networks (LSTM) are a specialized type of recurrent neural network (RNN) designed to effectively handle dependencies in long sequence data. Traditional RNNs often face the problem of forgetting when processing long time sequences, making it difficult to retain important information. LSTM addresses this issue by introducing a &ldquo;cell state&rdquo; mechanism that allows information to flow directly across time steps, helping the network better capture long-term dependencies."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"RNN and NLP Study Notes 04","item":"http://localhost:1313/posts/nlp04/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"RNN and NLP Study Notes 04","name":"RNN and NLP Study Notes 04","description":"Long Short Term Memory (LSTM) LSTM Model Long Short-Term Memory networks (LSTM) are a specialized type of recurrent neural network (RNN) designed to effectively handle dependencies in long sequence data. Traditional RNNs often face the problem of forgetting when processing long time sequences, making it difficult to retain important information. LSTM addresses this issue by introducing a \u0026ldquo;cell state\u0026rdquo; mechanism that allows information to flow directly across time steps, helping the network better capture long-term dependencies.\n","keywords":[""],"articleBody":"Long Short Term Memory (LSTM) LSTM Model Long Short-Term Memory networks (LSTM) are a specialized type of recurrent neural network (RNN) designed to effectively handle dependencies in long sequence data. Traditional RNNs often face the problem of forgetting when processing long time sequences, making it difficult to retain important information. LSTM addresses this issue by introducing a “cell state” mechanism that allows information to flow directly across time steps, helping the network better capture long-term dependencies.\n![image-20250326144600920](../../../../Library/Application Support/typora-user-images/image-20250326144600920.png)\nThe structure of LSTM is more complex than that of a standard simple RNN. Each of its repeating modules consists of multiple interacting neural network layers rather than just a single layer. Specifically, LSTM uses four parameter matrices to control the forgetting, updating, and output of information, which enhances its capability in modeling sequence data. The design of LSTM’s input gate, forget gate, and output gate ensures that it can actively select which information should be retained or updated, allowing it to excel in processing complex sequence tasks. This flexibility makes LSTM widely used in fields such as natural language processing and speech recognition, helping to solve many practical problems.\nStep-by-Step LSTM Walk Through The core of LSTM lies in its “cell state,” which is a horizontal line that runs through the entire network. The cell state can be thought of as a conveyor belt that flows directly through the network while engaging in only a small amount of linear interaction. This allows information to flow easily and remain unchanged along this conveyor belt.\n![image-20250326145155175](../../../../Library/Application Support/typora-user-images/image-20250326145155175.png)\nThe “gates” in LSTM are mechanisms that can selectively allow information to pass through. Each gate consists of a sigmoid neural network layer and a pointwise multiplication operation. The sigmoid layer outputs a value between 0 and 1, determining how much of each piece of information should be allowed to pass. A value of 0 means “no information is allowed to pass,” while a value of 1 means “all information is allowed to pass.”\n![image-20250326145807368](../../../../Library/Application Support/typora-user-images/image-20250326145807368.png)\nLSTM has three such gates, responsible for protecting and controlling the cell state, ensuring that information can flow and update effectively within the network.\nForget Gate The forget gate $f_t$ in Long Short-Term Memory (LSTM) networks is responsible for determining which information from the cell state should be discarded, consisting mainly of a sigmoid function and elementwise multiplication.\n![image-20250326151710194](../../../../Library/Application Support/typora-user-images/image-20250326151710194.png)\nFirst, the forget gate concatenates the previous hidden state vector $h$ with the current input vector $x$ to form a new vector. Then, it applies the sigmoid function to compress each element of this concatenated vector to a value between 0 and 1, determining the extent to which each piece of information should be retained.\n![image-20250326151436309](../../../../Library/Application Support/typora-user-images/image-20250326151436309.png)\nSubsequently, each element of the output vector $f_t$ generated by the forget gate is multiplied elementwise with the corresponding element of the cell state $c$. This means that if any element of $f_t$ is 0, the corresponding element of $c$ will be completely forgotten, resulting in an output of 0; conversely, if an element of $f_t$ is 1, the corresponding element of $c$ can fully pass through, maintaining its value. Therefore, the design of the forget gate allows the LSTM to flexibly choose which information to retain and which to discard.\n![image-20250326151451949](../../../../Library/Application Support/typora-user-images/image-20250326151451949.png)\nMathematically, the calculation of the forget gate is represented as: $$ f(t) = W_f \\cdot \\sigma([h_{t-1},x_t] + b_f) $$ Here, $W_f$ is the weight matrix of the forget gate, $\\sigma$ is the sigmoid activation function, $[h_{t-1}, x_t]$ represents the concatenation of the previous hidden state $h_{t-1}$ and the current input $x_t$, and $b_f$ is the bias term. In this way, the forget gate can effectively control the flow of information, ensuring the effective learning and stable operation of the LSTM network.\nInput Gate The input gate $i_t$ in Long Short-Term Memory (LSTM) networks decides which values from the conveyor belt will be updated. It relies on the previous hidden state $h_{t-1}$ and the current input $x_t$. The first step of the input gate is similar to that of the forget gate, where it concatenates $h_{t-1}$ and $x_t$, then applies the sigmoid function to obtain the output $i_t$, which ranges between (0, 1).\n![image-20250326161507245](../../../../Library/Application Support/typora-user-images/image-20250326161507245.png)\nMathematically, the calculation of the input gate is represented as: $$ i(t) = W_i \\cdot \\sigma([h_{t-1}, x_t] + b_i) $$ Here, $W_i$ is the weight matrix for the input gate, $\\sigma$ is the sigmoid activation function, and $b_i$ is the bias term.\nCandidate Memory Cell In addition to the input gate, we need to compute the new candidate value $\\tilde{c}_t$, which will be added to the conveyor belt. This computation is similar to that of $i_t$ but uses the $\\tanh$ function instead of the sigmoid function. Thus, the elements of $\\tilde{c}_t$ range between $(-1, 1)$.\n![image-20250326161749809](../../../../Library/Application Support/typora-user-images/image-20250326161749809.png)\nMathematically, the calculation of the candidate memory cell is represented as: $$ \\tilde{c}(t) = W_c \\cdot \\tanh([h_{t-1}, x_t] + b_c) $$ Here, $W_c$ is the weight matrix for the candidate memory cell, $\\tanh$ is the hyperbolic tangent activation function, and $b_c$ is the bias term.\nUpdating the Cell State With the known values of the previous cell state $C_{t-1}$, the output of the forget gate $f_t$, the output of the input gate $i_t$, and the candidate value $\\tilde{c}_t$, we can now proceed to update the cell state $C$.\nThe forget gate $f_t$ is multiplied elementwise with the old cell state $C_{t-1}$ to determine which elements to forget. If any element of $f_t$ is 0, the corresponding element of $C_{t-1}$ will be completely forgotten.\n![image-20250326162118762](../../../../Library/Application Support/typora-user-images/image-20250326162118762.png)\nNext, after selectively forgetting some elements in the cell state, we need to add new information to the conveyor belt. This is done by performing elementwise multiplication between the input gate output $i_t$ and the candidate value $\\tilde{c}_t$, followed by adding these two results together to produce the new cell state $C_t$.\n![image-20250326162147608](../../../../Library/Application Support/typora-user-images/image-20250326162147608.png)\nMathematically, the update of the cell state is represented as: $$ C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{c}_t $$ Where $\\odot$ denotes elementwise multiplication.\nThis completes one update cycle of the conveyor belt. After updating the cell state $C$, we need to compute the LSTM’s output $h_t$. First, we calculate the output gate $o_t$, which follows a similar procedure to that of the input gate and the forget gate.\nOutput Gate The output gate $o_t$ in Long Short-Term Memory (LSTM) networks decides what information flows from the conveyor belt $C_t$ to the hidden state $h_t$. It takes into account the previous hidden state $h_{t-1}$ and the current input $x_t$.\nThe first step involves concatenating $h_{t-1}$ and $x_t$, after which the concatenated vector is processed through a sigmoid activation function to produce $o_t$. The output $o_t$ ranges between $(0, 1)$, indicating the extent to which information should be kept from the cell state.\n![image-20250326163741544](../../../../Library/Application Support/typora-user-images/image-20250326163741544.png)\nMathematically, the calculation of the output gate is represented as: $$ o(t) = W_o \\cdot \\sigma([h_{t-1}, x_t] + b_o) $$ Here, $W_o$ is the weight matrix for the output gate, $\\sigma$ is the sigmoid activation function, and $b_o$ is the bias term.\nCalculating the Output State To compute the output state $h_t$, we first apply the hyperbolic tangent function ($\\tanh$) to the current cell state $C_t$. This compresses each element to a range of $(-1, 1)$. Then, we multiply the result elementwise by the output gate $o_t$.\n![image-20250326164043978](../../../../Library/Application Support/typora-user-images/image-20250326164043978.png)\nMathematically, this is expressed as: $$ h_t = o_t \\odot \\tanh(C_t) $$ Here, $o_t$ provides the gating mechanism that controls which parts of $C_t$ are passed to $h_t$ through elementwise multiplication $\\odot$.\nState Representation The output state $h_t$ serves as the final output of the LSTM. It has two copies: one forwarded to the next time step and the other used as the current output. By the time step $t$, all input vectors $x$ processed by the LSTM contribute to the information in $h_t$, enabling the network to effectively utilize historical context for its predictions.\nThis structure ensures that the LSTM can maintain relevant information across time steps while controlling what is outputted in each cycle.\nUnderstanding $C_t$ and $h_t$ in LSTM In Long Short-Term Memory (LSTM) networks, $C_t$ and $h_t$ are two critical state variables, each serving distinct purposes:\n$C_t$ (Cell State): Represents long-term memory within the LSTM structure. Used to carry information that helps the network remember long-term dependencies when processing sequential data. It is updated by retaining important information and forgetting unnecessary details, typically controlled by the forget gate and input gate. $C_t$ is a single vector that is continuously passed along through the time steps. $h_t$ (Hidden State): Represents short-term memory within the LSTM structure. Captures the output of the current time step, which can be used for predicting the next time step’s input. $h_t$ is derived from the cell state after passing through a nonlinear transformation (usually the $\\tanh$ function) and is sent to the network’s output layer. It can be viewed as a summary of the information at the current time step, serving as the output of the network at that moment. In summary, $C_t$ serves to store long-term memories, allowing the network to track information more meticulously, while $h_t$ is a short-term representation of that state, emphasizing the output at the current time step. Together, these states enable LSTMs to effectively process time series data and learn long-range dependencies.\nModel Parameters of LSTM When discussing the parameters of LSTM, attention should be paid to the dimensions of each parameter matrix. Specifically, there are a total of four parameter matrices in LSTM. The number of rows in each parameter matrix corresponds to the dimension of the hidden state $shape(h)$, while the number of columns is the sum of the hidden state dimension and the input dimension $shape(h) + shape(x)$. Therefore, the total number of parameters in these matrices can be represented as:\n#parameter matrices: 4 #rows in each matrix: $shape(h)$ #columns in each matrix: $shape(h) + shape(x)$ Total #parameters (not counting intercept): $4 \\times shape(h) \\times [shape(h) + shape(x)]$ This means that as the input scale increases, the number of parameters in LSTM will also grow accordingly, thereby influencing the model’s complexity and training requirements.\nSummary LSTM uses a “conveyor belt” to get longer memory than SimpleRNN.\nEach of the following blocks has a parameter matrix:\nForget gate. Input gate. New values. Output gate. Number of parameters: $$ 4 \\times shape(h) \\times [shape(h) + shape(x)] $$\ndropout也可以用在LSTM上\n","wordCount":"1718","inLanguage":"en","datePublished":"2025-03-24T11:41:35+08:00","dateModified":"2025-03-24T11:41:35+08:00","author":{"@type":"Person","name":"Tristan"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/nlp04/"},"publisher":{"@type":"Organization","name":"Tristan's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Tristan's Blog (Alt + H)">Tristan's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/resume/ title=Resume><span>Resume</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">RNN and NLP Study Notes 04
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentcolor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2025-03-24 11:41:35 +0800 HKT'>March 24, 2025</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1718 words&nbsp;·&nbsp;Tristan&nbsp;|&nbsp;<a href=https://github.com/BaizeXS/baizexs.github.io/tree/main/content/posts/NLP04.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#long-short-term-memory-lstm>Long Short Term Memory (LSTM)</a><ul><li><a href=#lstm-model>LSTM Model</a></li><li><a href=#step-by-step-lstm-walk-through>Step-by-Step LSTM Walk Through</a></li><li><a href=#understanding-c_t-and-h_t-in-lstm>Understanding $C_t$ and $h_t$ in LSTM</a></li><li><a href=#model-parameters-of-lstm>Model Parameters of LSTM</a></li><li><a href=#summary>Summary</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=long-short-term-memory-lstm>Long Short Term Memory (LSTM)<a hidden class=anchor aria-hidden=true href=#long-short-term-memory-lstm>#</a></h2><h3 id=lstm-model>LSTM Model<a hidden class=anchor aria-hidden=true href=#lstm-model>#</a></h3><p>Long Short-Term Memory networks (LSTM) are a specialized type of recurrent neural network (RNN) designed to effectively handle dependencies in long sequence data. Traditional RNNs often face the problem of forgetting when processing long time sequences, making it difficult to retain important information. LSTM addresses this issue by introducing a &ldquo;cell state&rdquo; mechanism that allows information to flow directly across time steps, helping the network better capture long-term dependencies.</p><p>![image-20250326144600920](../../../../Library/Application Support/typora-user-images/image-20250326144600920.png)</p><p>The structure of LSTM is more complex than that of a standard simple RNN. Each of its repeating modules consists of multiple interacting neural network layers rather than just a single layer. Specifically, LSTM uses four parameter matrices to control the forgetting, updating, and output of information, which enhances its capability in modeling sequence data. The design of LSTM&rsquo;s input gate, forget gate, and output gate ensures that it can actively select which information should be retained or updated, allowing it to excel in processing complex sequence tasks. This flexibility makes LSTM widely used in fields such as natural language processing and speech recognition, helping to solve many practical problems.</p><h3 id=step-by-step-lstm-walk-through>Step-by-Step LSTM Walk Through<a hidden class=anchor aria-hidden=true href=#step-by-step-lstm-walk-through>#</a></h3><p>The core of LSTM lies in its &ldquo;cell state,&rdquo; which is a horizontal line that runs through the entire network. The cell state can be thought of as a conveyor belt that flows directly through the network while engaging in only a small amount of linear interaction. This allows information to flow easily and remain unchanged along this conveyor belt.</p><p>![image-20250326145155175](../../../../Library/Application Support/typora-user-images/image-20250326145155175.png)</p><p>The &ldquo;gates&rdquo; in LSTM are mechanisms that can selectively allow information to pass through. Each gate consists of a sigmoid neural network layer and a pointwise multiplication operation. The sigmoid layer outputs a value between 0 and 1, determining how much of each piece of information should be allowed to pass. A value of 0 means &ldquo;no information is allowed to pass,&rdquo; while a value of 1 means &ldquo;all information is allowed to pass.”</p><p>![image-20250326145807368](../../../../Library/Application Support/typora-user-images/image-20250326145807368.png)</p><p>LSTM has three such gates, responsible for protecting and controlling the cell state, ensuring that information can flow and update effectively within the network.</p><h4 id=forget-gate>Forget Gate<a hidden class=anchor aria-hidden=true href=#forget-gate>#</a></h4><p>The forget gate $f_t$ in Long Short-Term Memory (LSTM) networks is responsible for determining which information from the cell state should be discarded, consisting mainly of a sigmoid function and elementwise multiplication.</p><p>![image-20250326151710194](../../../../Library/Application Support/typora-user-images/image-20250326151710194.png)</p><p>First, the forget gate concatenates the previous hidden state vector $h$ with the current input vector $x$ to form a new vector. Then, it applies the sigmoid function to compress each element of this concatenated vector to a value between 0 and 1, determining the extent to which each piece of information should be retained.</p><p>![image-20250326151436309](../../../../Library/Application Support/typora-user-images/image-20250326151436309.png)</p><p>Subsequently, each element of the output vector $f_t$ generated by the forget gate is multiplied elementwise with the corresponding element of the cell state $c$. This means that if any element of $f_t$ is 0, the corresponding element of $c$ will be completely forgotten, resulting in an output of 0; conversely, if an element of $f_t$ is 1, the corresponding element of $c$ can fully pass through, maintaining its value. Therefore, the design of the forget gate allows the LSTM to flexibly choose which information to retain and which to discard.</p><p>![image-20250326151451949](../../../../Library/Application Support/typora-user-images/image-20250326151451949.png)</p><p>Mathematically, the calculation of the forget gate is represented as:
$$
f(t) = W_f \cdot \sigma([h_{t-1},x_t] + b_f)
$$
Here, $W_f$ is the weight matrix of the forget gate, $\sigma$ is the sigmoid activation function, $[h_{t-1}, x_t]$ represents the concatenation of the previous hidden state $h_{t-1}$ and the current input $x_t$, and $b_f$ is the bias term. In this way, the forget gate can effectively control the flow of information, ensuring the effective learning and stable operation of the LSTM network.</p><h4 id=input-gate>Input Gate<a hidden class=anchor aria-hidden=true href=#input-gate>#</a></h4><p>The input gate $i_t$ in Long Short-Term Memory (LSTM) networks decides which values from the conveyor belt will be updated. It relies on the previous hidden state $h_{t-1}$ and the current input $x_t$. The first step of the input gate is similar to that of the forget gate, where it concatenates $h_{t-1}$ and $x_t$, then applies the sigmoid function to obtain the output $i_t$, which ranges between (0, 1).</p><p>![image-20250326161507245](../../../../Library/Application Support/typora-user-images/image-20250326161507245.png)</p><p>Mathematically, the calculation of the input gate is represented as:
$$
i(t) = W_i \cdot \sigma([h_{t-1}, x_t] + b_i)
$$
Here, $W_i$ is the weight matrix for the input gate, $\sigma$ is the sigmoid activation function, and $b_i$ is the bias term.</p><h4 id=candidate-memory-cell>Candidate Memory Cell<a hidden class=anchor aria-hidden=true href=#candidate-memory-cell>#</a></h4><p>In addition to the input gate, we need to compute the new candidate value $\tilde{c}_t$, which will be added to the conveyor belt. This computation is similar to that of $i_t$ but uses the $\tanh$ function instead of the sigmoid function. Thus, the elements of $\tilde{c}_t$ range between $(-1, 1)$.</p><p>![image-20250326161749809](../../../../Library/Application Support/typora-user-images/image-20250326161749809.png)</p><p>Mathematically, the calculation of the candidate memory cell is represented as:
$$
\tilde{c}(t) = W_c \cdot \tanh([h_{t-1}, x_t] + b_c)
$$
Here, $W_c$ is the weight matrix for the candidate memory cell, $\tanh$ is the hyperbolic tangent activation function, and $b_c$ is the bias term.</p><h4 id=updating-the-cell-state>Updating the Cell State<a hidden class=anchor aria-hidden=true href=#updating-the-cell-state>#</a></h4><p>With the known values of the previous cell state $C_{t-1}$, the output of the forget gate $f_t$, the output of the input gate $i_t$, and the candidate value $\tilde{c}_t$, we can now proceed to update the cell state $C$.</p><p>The forget gate $f_t$ is multiplied elementwise with the old cell state $C_{t-1}$ to determine which elements to forget. If any element of $f_t$ is 0, the corresponding element of $C_{t-1}$ will be completely forgotten.</p><p>![image-20250326162118762](../../../../Library/Application Support/typora-user-images/image-20250326162118762.png)</p><p>Next, after selectively forgetting some elements in the cell state, we need to add new information to the conveyor belt. This is done by performing elementwise multiplication between the input gate output $i_t$ and the candidate value $\tilde{c}_t$, followed by adding these two results together to produce the new cell state $C_t$.</p><p>![image-20250326162147608](../../../../Library/Application Support/typora-user-images/image-20250326162147608.png)</p><p>Mathematically, the update of the cell state is represented as:
$$
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{c}_t
$$
Where $\odot$ denotes elementwise multiplication.</p><p>This completes one update cycle of the conveyor belt. After updating the cell state $C$, we need to compute the LSTM&rsquo;s output $h_t$. First, we calculate the output gate $o_t$, which follows a similar procedure to that of the input gate and the forget gate.</p><h4 id=output-gate>Output Gate<a hidden class=anchor aria-hidden=true href=#output-gate>#</a></h4><p>The output gate $o_t$ in Long Short-Term Memory (LSTM) networks decides what information flows from the conveyor belt $C_t$ to the hidden state $h_t$. It takes into account the previous hidden state $h_{t-1}$ and the current input $x_t$.</p><p>The first step involves concatenating $h_{t-1}$ and $x_t$, after which the concatenated vector is processed through a sigmoid activation function to produce $o_t$. The output $o_t$ ranges between $(0, 1)$, indicating the extent to which information should be kept from the cell state.</p><p>![image-20250326163741544](../../../../Library/Application Support/typora-user-images/image-20250326163741544.png)</p><p>Mathematically, the calculation of the output gate is represented as:
$$
o(t) = W_o \cdot \sigma([h_{t-1}, x_t] + b_o)
$$
Here, $W_o$ is the weight matrix for the output gate, $\sigma$ is the sigmoid activation function, and $b_o$ is the bias term.</p><h4 id=calculating-the-output-state>Calculating the Output State<a hidden class=anchor aria-hidden=true href=#calculating-the-output-state>#</a></h4><p>To compute the output state $h_t$, we first apply the hyperbolic tangent function ($\tanh$) to the current cell state $C_t$. This compresses each element to a range of $(-1, 1)$. Then, we multiply the result elementwise by the output gate $o_t$.</p><p>![image-20250326164043978](../../../../Library/Application Support/typora-user-images/image-20250326164043978.png)</p><p>Mathematically, this is expressed as:
$$
h_t = o_t \odot \tanh(C_t)
$$
Here, $o_t$ provides the gating mechanism that controls which parts of $C_t$ are passed to $h_t$ through elementwise multiplication $\odot$.</p><h4 id=state-representation>State Representation<a hidden class=anchor aria-hidden=true href=#state-representation>#</a></h4><p>The output state $h_t$ serves as the final output of the LSTM. It has two copies: one forwarded to the next time step and the other used as the current output. By the time step $t$, all input vectors $x$ processed by the LSTM contribute to the information in $h_t$, enabling the network to effectively utilize historical context for its predictions.</p><p>This structure ensures that the LSTM can maintain relevant information across time steps while controlling what is outputted in each cycle.</p><h3 id=understanding-c_t-and-h_t-in-lstm>Understanding $C_t$ and $h_t$ in LSTM<a hidden class=anchor aria-hidden=true href=#understanding-c_t-and-h_t-in-lstm>#</a></h3><p>In Long Short-Term Memory (LSTM) networks, $C_t$ and $h_t$ are two critical state variables, each serving distinct purposes:</p><ol><li>$C_t$ (Cell State):<ul><li>Represents <strong>long-term memory</strong> within the LSTM structure.</li><li>Used to carry information that helps the network remember long-term dependencies when processing sequential data.</li><li>It is updated by <strong>retaining important information</strong> and <strong>forgetting unnecessary</strong> details, typically controlled by the forget gate and input gate.</li><li>$C_t$ is a single vector that is continuously passed along through the time steps.</li></ul></li><li>$h_t$ (Hidden State):<ul><li>Represents <strong>short-term memory</strong> within the LSTM structure.</li><li>Captures the output of the current time step, which can be used for predicting the next time step&rsquo;s input.</li><li>$h_t$ is derived from the cell state after passing through a nonlinear transformation (usually the $\tanh$ function) and is sent to the network&rsquo;s output layer.</li><li>It can be viewed as a summary of the information at the current time step, serving as the output of the network at that moment.</li></ul></li></ol><p>In summary, $C_t$ serves to store long-term memories, allowing the network to track information more meticulously, while $h_t$ is a short-term representation of that state, emphasizing the output at the current time step. Together, these states enable LSTMs to effectively process time series data and learn long-range dependencies.</p><h3 id=model-parameters-of-lstm>Model Parameters of LSTM<a hidden class=anchor aria-hidden=true href=#model-parameters-of-lstm>#</a></h3><p>When discussing the parameters of LSTM, attention should be paid to the dimensions of each parameter matrix. Specifically, there are a total of <strong>four parameter matrices</strong> in LSTM. The number of rows in each parameter matrix corresponds to the dimension of the hidden state $shape(h)$, while the number of columns is the sum of the hidden state dimension and the input dimension $shape(h) + shape(x)$. Therefore, the total number of parameters in these matrices can be represented as:</p><ul><li>#parameter matrices: 4</li><li>#rows in each matrix: $shape(h)$</li><li>#columns in each matrix: $shape(h) + shape(x)$</li><li>Total #parameters (not counting intercept): $4 \times shape(h) \times [shape(h) + shape(x)]$</li></ul><p>This means that as the input scale increases, the number of parameters in LSTM will also grow accordingly, thereby influencing the model&rsquo;s complexity and training requirements.</p><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><ul><li><p>LSTM uses a “conveyor belt” to get longer memory than SimpleRNN.</p></li><li><p>Each of the following blocks has a parameter matrix:</p><ul><li>Forget gate.</li><li>Input gate.</li><li>New values.</li><li>Output gate.</li></ul></li><li><p>Number of parameters:
$$
4 \times shape(h) \times [shape(h) + shape(x)]
$$</p></li><li><p>dropout也可以用在LSTM上</p></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/nlp05/><span class=title>« Prev</span><br><span>RNN and NLP Study Notes 05</span>
</a><a class=next href=http://localhost:1313/posts/nlp03/><span class=title>Next »</span><br><span>RNN and NLP Study Notes 03</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 04 on x" href="https://x.com/intent/tweet/?text=RNN%20and%20NLP%20Study%20Notes%2004&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp04%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 04 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp04%2f&amp;title=RNN%20and%20NLP%20Study%20Notes%2004&amp;summary=RNN%20and%20NLP%20Study%20Notes%2004&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp04%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 04 on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp04%2f&title=RNN%20and%20NLP%20Study%20Notes%2004"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 04 on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp04%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 04 on whatsapp" href="https://api.whatsapp.com/send?text=RNN%20and%20NLP%20Study%20Notes%2004%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp04%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 04 on telegram" href="https://telegram.me/share/url?text=RNN%20and%20NLP%20Study%20Notes%2004&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp04%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 04 on ycombinator" href="https://news.ycombinator.com/submitlink?t=RNN%20and%20NLP%20Study%20Notes%2004&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp04%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Tristan's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>