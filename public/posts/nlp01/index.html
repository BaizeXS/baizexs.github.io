<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>RNN and NLP Study Notes 01 | Tristan's Blog</title>
<meta name=keywords content="Machine Learning,NLP,ML Basics"><meta name=description content="Data Processing Basics
In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:

  
      
          Age
          Gender
          Nationality
      
  
  
      
          35
          Male
          US
      
      
          31
          Male
          China
      
      
          29
          Female
          India
      
      
          27
          Male
          US
      
  

Numeric Features
Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person&rsquo;s age serves as a numeric feature, where 35 is greater than 31."><meta name=author content="Tristan"><link rel=canonical href=http://localhost:1313/posts/nlp01/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.392aecb5c92792e8888f6bee404f8c562ed37691c3e5e0914aa5cd0a29ec8350.css integrity="sha256-OSrstcknkuiIj2vuQE+MVi7TdpHD5eCRSqXNCinsg1A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/favicon/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/nlp01/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:url" content="http://localhost:1313/posts/nlp01/"><meta property="og:site_name" content="Tristan's Blog"><meta property="og:title" content="RNN and NLP Study Notes 01"><meta property="og:description" content="Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:
Age Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person’s age serves as a numeric feature, where 35 is greater than 31."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-23T21:43:18+08:00"><meta property="article:modified_time" content="2025-03-23T21:43:18+08:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="NLP"><meta property="article:tag" content="ML Basics"><meta name=twitter:card content="summary"><meta name=twitter:title content="RNN and NLP Study Notes 01"><meta name=twitter:description content="Data Processing Basics
In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:

  
      
          Age
          Gender
          Nationality
      
  
  
      
          35
          Male
          US
      
      
          31
          Male
          China
      
      
          29
          Female
          India
      
      
          27
          Male
          US
      
  

Numeric Features
Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person&rsquo;s age serves as a numeric feature, where 35 is greater than 31."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"RNN and NLP Study Notes 01","item":"http://localhost:1313/posts/nlp01/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"RNN and NLP Study Notes 01","name":"RNN and NLP Study Notes 01","description":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\n","keywords":["Machine Learning","NLP","ML Basics"],"articleBody":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person’s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent “unknown” or “missing,” which corresponds to a vector of all 0s. This enhances the model’s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person’s Features When analyzing a person’s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let’s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person’s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nReferences For a deeper understanding of these concepts, you can refer to the following course video:RNN模型与NLP应用(1/9)：数据处理基础.\n","wordCount":"1125","inLanguage":"en","datePublished":"2025-03-23T21:43:18+08:00","dateModified":"2025-03-23T21:43:18+08:00","author":{"@type":"Person","name":"Tristan"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/nlp01/"},"publisher":{"@type":"Organization","name":"Tristan's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Tristan's Blog (Alt + H)">Tristan's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/resume/ title=Resume><span>Resume</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">RNN and NLP Study Notes 01</h1><div class=post-meta><span title='2025-03-23 21:43:18 +0800 HKT'>March 23, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1125 words&nbsp;·&nbsp;Tristan&nbsp;|&nbsp;<a href=https://github.com/BaizeXS/baizexs.github.io/tree/main/content/posts/NLP01.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#data-processing-basics>Data Processing Basics</a><ul><li><a href=#numeric-features>Numeric Features</a></li><li><a href=#binary-features>Binary Features</a></li><li><a href=#categorical-features>Categorical Features</a></li><li><a href=#issue-with-categorical-features>Issue with Categorical Features</a></li><li><a href=#solution-one-hot-encoding>Solution: One-Hot Encoding</a></li></ul></li><li><a href=#one-hot-encoding>One-Hot Encoding</a><ul><li><a href=#why-use-one-hot-encoding>Why Use One-Hot Encoding</a></li><li><a href=#steps-of-one-hot-encoding>Steps of One-Hot Encoding</a></li><li><a href=#example-representing-a-persons-features>Example: Representing a Person&rsquo;s Features</a></li></ul></li><li><a href=#processing-text-data>Processing Text Data</a><ul><li><a href=#step-1-tokenization-text-to-words>Step 1: Tokenization (Text to Words)</a></li><li><a href=#step-2-count-word-frequencies>Step 2: Count Word Frequencies</a></li><li><a href=#step-3-limit-vocabulary-size>Step 3: Limit Vocabulary Size</a></li><li><a href=#step-4-one-hot-encoding>Step 4: One-Hot Encoding</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><h2 id=data-processing-basics>Data Processing Basics<a hidden class=anchor aria-hidden=true href=#data-processing-basics>#</a></h2><p>In machine learning, data types can be classified into several forms, including <strong>numeric features</strong>, <strong>categorical features</strong>, and <strong>binary features</strong>. The table below provides concrete examples to better understand these data types:</p><table><thead><tr><th>Age</th><th>Gender</th><th>Nationality</th></tr></thead><tbody><tr><td>35</td><td>Male</td><td>US</td></tr><tr><td>31</td><td>Male</td><td>China</td></tr><tr><td>29</td><td>Female</td><td>India</td></tr><tr><td>27</td><td>Male</td><td>US</td></tr></tbody></table><h3 id=numeric-features>Numeric Features<a hidden class=anchor aria-hidden=true href=#numeric-features>#</a></h3><p>Numeric features refer to data that possess <strong>additive properties</strong> and <strong>can be compared in magnitude</strong>. For example, a person&rsquo;s age serves as a numeric feature, where 35 is greater than 31.</p><h3 id=binary-features>Binary Features<a hidden class=anchor aria-hidden=true href=#binary-features>#</a></h3><p>Binary features typically represent scenarios with <strong>only two possible values</strong>, such as gender. Gender can be represented by 0 (female) and 1 (male).</p><h3 id=categorical-features>Categorical Features<a hidden class=anchor aria-hidden=true href=#categorical-features>#</a></h3><p>Categorical features are used to represent <strong>different categories</strong>, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as <code>US=1</code>, <code>China=2</code>, <code>India=3</code>, and so on.</p><h3 id=issue-with-categorical-features>Issue with Categorical Features<a hidden class=anchor aria-hidden=true href=#issue-with-categorical-features>#</a></h3><p>Representing categorical features using integer values may lead to <strong>incorrect numerical relationships</strong>. For instance, the expression <code>US + China = India</code> is ridiculous.</p><h3 id=solution-one-hot-encoding>Solution: One-Hot Encoding<a hidden class=anchor aria-hidden=true href=#solution-one-hot-encoding>#</a></h3><p>One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:</p><ul><li>The U.S. is represented as <code>[1, 0, 0, 0, ..., 0]</code>.</li><li>China is represented as <code>[0, 1, 0, 0, ..., 0]</code>.</li><li>India is represented as <code>[0, 0, 1, 0, …, 0]</code>.</li></ul><p>In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: <code>[1, 0, 1, 0, ..., 0]</code>. This indicates that they possess both nationalities.</p><hr><h2 id=one-hot-encoding>One-Hot Encoding<a hidden class=anchor aria-hidden=true href=#one-hot-encoding>#</a></h2><p>In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.</p><h3 id=why-use-one-hot-encoding>Why Use One-Hot Encoding<a hidden class=anchor aria-hidden=true href=#why-use-one-hot-encoding>#</a></h3><ul><li><strong>Avoiding Pseudo-Numeric Relationships</strong>: One-Hot encoding prevents <strong>false numeric relationships</strong> that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values.</li><li><strong>Complete Information Retention</strong>: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories.</li></ul><h3 id=steps-of-one-hot-encoding>Steps of One-Hot Encoding<a hidden class=anchor aria-hidden=true href=#steps-of-one-hot-encoding>#</a></h3><p>One-Hot encoding is typically performed in two main steps:</p><ol><li><p><strong>Establishing Mapping: From Category to Index</strong></p><ul><li>Assign a unique index to each category, typically starting from 1.</li><li>Reserve index 0 to represent &ldquo;unknown&rdquo; or &ldquo;missing,&rdquo; which corresponds to a vector of all 0s. This enhances the model&rsquo;s tolerance for missing data.</li></ul></li><li><p><strong>Converting to One-Hot Vector</strong></p><ul><li><p>Each category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.</p></li><li><p>For example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:</p><ul><li><p>The United States can be represented as <code>[1, 0, 0, 0, ...]</code>, indicating its position in the vector.</p></li><li><p>China can be represented as <code>[0, 1, 0, 0, ...]</code>, indicating its position as well.</p></li></ul></li></ul></li></ol><p>These steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.</p><h3 id=example-representing-a-persons-features>Example: Representing a Person&rsquo;s Features<a hidden class=anchor aria-hidden=true href=#example-representing-a-persons-features>#</a></h3><p>When analyzing a person&rsquo;s characteristics, several key aspects come into play, including <strong>age</strong>, <strong>gender</strong>, and <strong>nationality</strong>. For our purposes, let&rsquo;s assume there are 197 nationalities.</p><p>To calculate the total number of feature dimensions, we can break it down as follows:</p><ul><li><strong>Age</strong>: 1 dimension</li><li><strong>Gender</strong>: 1 dimension</li><li><strong>Nationality</strong>: 197 dimensions</li></ul><p>This gives us a total of <strong>199 dimensions</strong> to represent a person&rsquo;s features.</p><p>For example, consider a person with the attributes <code>(28, Female, China)</code>. The features can be represented by the following One-Hot vector:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>[28, 0, 0, 1, 0, ..., 0]  
</span></span></code></pre></td></tr></table></div></div><p>In this vector:</p><ul><li>The age retains its original value of 28.</li><li>Gender is represented by 0 (indicating female).</li><li>The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved.</li></ul><p>This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.</p><hr><h2 id=processing-text-data>Processing Text Data<a hidden class=anchor aria-hidden=true href=#processing-text-data>#</a></h2><p>Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.</p><h3 id=step-1-tokenization-text-to-words>Step 1: Tokenization (Text to Words)<a hidden class=anchor aria-hidden=true href=#step-1-tokenization-text-to-words>#</a></h3><p>The first step in processing text data is <strong>tokenization</strong>, where a string of text is broken down into a list of individual words. For example, given the text:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>S = “… to be or not to be…”
</span></span></code></pre></td></tr></table></div></div><p>We break this string into a list of words:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>L = [..., to, be, or, not, to, be, ...]
</span></span></code></pre></td></tr></table></div></div><h3 id=step-2-count-word-frequencies>Step 2: Count Word Frequencies<a hidden class=anchor aria-hidden=true href=#step-2-count-word-frequencies>#</a></h3><p>Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.</p><ul><li>Initially, the dictionary is empty.</li><li>For each word ($w$):<ul><li>If the $w$ is not in the dictionary, add $w$ with a frequency of 1.</li><li>If $w$ is in the dictionary, increment its frequency counter.</li></ul></li></ul><p>For example, after processing some text, the dictionary might look like this:</p><table><thead><tr><th>Key(word)</th><th>Value(frequency)</th></tr></thead><tbody><tr><td>a</td><td>219</td></tr><tr><td>to</td><td>398</td></tr><tr><td>hamlet</td><td>5</td></tr><tr><td>be</td><td>131</td></tr><tr><td>not</td><td>499</td></tr><tr><td>prince</td><td>12</td></tr><tr><td>kill</td><td>31</td></tr></tbody></table><h3 id=step-3-limit-vocabulary-size>Step 3: Limit Vocabulary Size<a hidden class=anchor aria-hidden=true href=#step-3-limit-vocabulary-size>#</a></h3><p>If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:</p><ol><li>Infrequent words often add little meaningful information, like typos or rare names.</li><li>A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer.</li></ol><h3 id=step-4-one-hot-encoding>Step 4: One-Hot Encoding<a hidden class=anchor aria-hidden=true href=#step-4-one-hot-encoding>#</a></h3><p>In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Words: [to, be, or, not, to, be]
</span></span></code></pre></td></tr></table></div></div><p>the corresponding indices might be:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Indices: [2, 4, 8, 1, 2, 4]
</span></span></code></pre></td></tr></table></div></div><p>Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>For a deeper understanding of these concepts, you can refer to the following course video:<a href="https://youtu.be/NWcShtqr8kc?si=OobtdifZSl41e_Mn">RNN模型与NLP应用(1/9)：数据处理基础</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/machine-learning/>Machine Learning</a></li><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li><li><a href=http://localhost:1313/tags/ml-basics/>ML Basics</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/nlp02/><span class=title>« Prev</span><br><span>RNN and NLP Study Notes 02</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 01 on x" href="https://x.com/intent/tweet/?text=RNN%20and%20NLP%20Study%20Notes%2001&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp01%2f&amp;hashtags=MachineLearning%2cNLP%2cMLBasics"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 01 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp01%2f&amp;title=RNN%20and%20NLP%20Study%20Notes%2001&amp;summary=RNN%20and%20NLP%20Study%20Notes%2001&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp01%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 01 on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp01%2f&title=RNN%20and%20NLP%20Study%20Notes%2001"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 01 on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp01%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 01 on whatsapp" href="https://api.whatsapp.com/send?text=RNN%20and%20NLP%20Study%20Notes%2001%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp01%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 01 on telegram" href="https://telegram.me/share/url?text=RNN%20and%20NLP%20Study%20Notes%2001&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp01%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RNN and NLP Study Notes 01 on ycombinator" href="https://news.ycombinator.com/submitlink?t=RNN%20and%20NLP%20Study%20Notes%2001&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fnlp01%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Tristan's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>