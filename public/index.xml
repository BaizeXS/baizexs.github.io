<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Tristan&#39;s Blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Tristan&#39;s Blog</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Mar 2025 11:41:56 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NLP09</title>
      <link>http://localhost:1313/posts/nlp09/</link>
      <pubDate>Mon, 24 Mar 2025 11:41:56 +0800</pubDate>
      <guid>http://localhost:1313/posts/nlp09/</guid>
      <description></description>
    </item>
    <item>
      <title>NLP08</title>
      <link>http://localhost:1313/posts/nlp08/</link>
      <pubDate>Mon, 24 Mar 2025 11:41:52 +0800</pubDate>
      <guid>http://localhost:1313/posts/nlp08/</guid>
      <description></description>
    </item>
    <item>
      <title>NLP07</title>
      <link>http://localhost:1313/posts/nlp07/</link>
      <pubDate>Mon, 24 Mar 2025 11:41:49 +0800</pubDate>
      <guid>http://localhost:1313/posts/nlp07/</guid>
      <description></description>
    </item>
    <item>
      <title>NLP06</title>
      <link>http://localhost:1313/posts/nlp06/</link>
      <pubDate>Mon, 24 Mar 2025 11:41:43 +0800</pubDate>
      <guid>http://localhost:1313/posts/nlp06/</guid>
      <description></description>
    </item>
    <item>
      <title>RNN and NLP Study Notes 05</title>
      <link>http://localhost:1313/posts/nlp05/</link>
      <pubDate>Mon, 24 Mar 2025 11:41:39 +0800</pubDate>
      <guid>http://localhost:1313/posts/nlp05/</guid>
      <description>&lt;h2 id=&#34;making-rnns-more-effective&#34;&gt;Making RNNs More Effective&lt;/h2&gt;
&lt;p&gt;包含三个方面：Stacked RNN、Bidirectional RNN、Pretrain&lt;/p&gt;
&lt;h3 id=&#34;stacked-rnn&#34;&gt;Stacked RNN&lt;/h3&gt;
&lt;p&gt;可以将很多全连接层堆叠起来，构成一个MLP；可以把很多卷积层堆叠起来，构成一个深度卷积网络；同样的道理，你可以将很多RNN堆叠起来，构成一个多层RNN网络；神经网络每一步都会更新状态h，新算出来的h有两个copies；一个送到下一个时刻，另一个作为输出，这一层的输出h成为了下一层的输入；&lt;/p&gt;
&lt;p&gt;最底层的RNN的输入为词向量x；这些词向量的输出h会作为下一层RNN的输入；&lt;/p&gt;
&lt;p&gt;![image-20250325234633209](../../../../Library/Application Support/typora-user-images/image-20250325234633209.png)&lt;/p&gt;
&lt;p&gt;最上层的状态为RNN的输出；最上层的最后一个状态h_t为最终的状态h_t，&lt;/p&gt;
&lt;h3 id=&#34;bidirectional-rnn&#34;&gt;Bidirectional RNN&lt;/h3&gt;
&lt;p&gt;Standard RNN和人的阅读习惯一样，从左往右阅读；人阅读的过程在脑子里积累阅读的信息；RNN在状态向量h_t中积累阅读到的信息；读完一份电影评论则可以知道是正面评价还是负面评价；我们人类总是从前往后，从左往右进行阅读；但是这只是人类的阅读习惯，从后往前阅读依旧可以判断电影评价是正面的还是负面的；对RNN来说，从前往后阅读或者从后往前阅读，是没有太大区别的，训练一个从后往前阅读的RNN，也会有一个比较不错的结果；所以比较自然的想法就是训练两条RNN，一条从左往右读，一条从右往左读；两条RNN是独立的，不共享参数，也不共享状态；两条RNN各自输出自己的状态向量，然后把它们的状态向量做cat；记做向量y；如果有多层RNN，就将这些y作为更上一层RNN的输入；如果只有一层RNN，则可以将y丢弃，只保留两条链最后的状态h_t和h_t‘；把这两个向量的cat作为从输入文本中提取出的特征向量；把这个向量用于判断是正面还是负面；&lt;/p&gt;
&lt;p&gt;![image-20250326000815290](../../../../Library/Application Support/typora-user-images/image-20250326000815290.png)&lt;/p&gt;
&lt;p&gt;双向RNN总是比单向RNN效果好；原因可能是这样的，无论哪个方向，或多或少会忘记最开始的输入；从左往右阅读可能忘记最左边的内容；反之亦然；&lt;/p&gt;
&lt;h3 id=&#34;pretrain&#34;&gt;Pretrain&lt;/h3&gt;
&lt;p&gt;预训练在深度学习中非常好用，比如在卷积神经网络中，如果网络太大而训练集不够大，那么可以在ImageNet大数据上做预训练；这样可以让神经网络有比较好的初始化，也可以避免Overfitting；训练RNN同理；例如这里Embedding层有320000参数，而我们只有2w个训练样本；这个Embedding太大了，会导致overfitting；解决办法就是对Embedding层做预训练；&lt;/p&gt;
&lt;p&gt;![image-20250326001446063](../../../../Library/Application Support/typora-user-images/image-20250326001446063.png)&lt;/p&gt;
&lt;p&gt;预训练具体如下；首先找一个更大的数据集，可以是情感分析的数据，也可以是其他类型的数据；但是任务最好是接近原来情感分析的任务；最好学习到的词向量带有正面或者负面情感；两个任务越相似，预训练之后的transfer就会越好；有了大数据集之后，要搭建一个神经网络，这个神经网络的结构是什么样都可以，甚至不用是RNN，只要这个神经网络有embedding层即可；然后就是在大数据集上训练这个神经网络；&lt;/p&gt;
&lt;p&gt;![image-20250326001629650](../../../../Library/Application Support/typora-user-images/image-20250326001629650.png)&lt;/p&gt;
&lt;p&gt;训练好之后，丢掉上面的层，只保留Embedding层和训练好的模型参数；&lt;/p&gt;
&lt;p&gt;![image-20250326002121770](../../../../Library/Application Support/typora-user-images/image-20250326002121770.png)&lt;/p&gt;
&lt;p&gt;然后再搭我们自己的RNN网络；这个新的RNN网络和之前预训练的可以有不同的结构，搭好之后，新的RNN层和全连接层都是随机初始化的，而下面的Embedding层的参数是预训练出来的，要固定住Embedding层的参数，不训练Embedding，只训练其他层；&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SimpleRNN and LSTM are two kinds of RNNs; always use LSTM instead of SimpleRNN.&lt;/li&gt;
&lt;li&gt;Use Bi-RNN instead of RNN whenever possible.&lt;/li&gt;
&lt;li&gt;Stacked RNN may be better than a single RNN layer (if $n$ is big).&lt;/li&gt;
&lt;li&gt;Pretrain the embedding layer (if $n$ is small).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;还有一些其他的RNN替代，例如GRU，Gated R Unit；但是不一定比LSTM好。&lt;/p&gt;</description>
    </item>
    <item>
      <title>RNN and NLP Study Notes 04</title>
      <link>http://localhost:1313/posts/nlp04/</link>
      <pubDate>Mon, 24 Mar 2025 11:41:35 +0800</pubDate>
      <guid>http://localhost:1313/posts/nlp04/</guid>
      <description>&lt;h2 id=&#34;long-short-term-memory-lstm&#34;&gt;Long Short Term Memory (LSTM)&lt;/h2&gt;
&lt;h3 id=&#34;lstm-model&#34;&gt;LSTM Model&lt;/h3&gt;
&lt;p&gt;Long Short-Term Memory networks (LSTM) are a specialized type of recurrent neural network (RNN) designed to effectively handle dependencies in long sequence data. Traditional RNNs often face the problem of forgetting when processing long time sequences, making it difficult to retain important information. LSTM addresses this issue by introducing a &amp;ldquo;cell state&amp;rdquo; mechanism that allows information to flow directly across time steps, helping the network better capture long-term dependencies.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RNN and NLP Study Notes 03</title>
      <link>http://localhost:1313/posts/nlp03/</link>
      <pubDate>Mon, 24 Mar 2025 11:41:32 +0800</pubDate>
      <guid>http://localhost:1313/posts/nlp03/</guid>
      <description>&lt;h2 id=&#34;recurrent-neural-networks-rnns&#34;&gt;Recurrent Neural Networks (RNNs)&lt;/h2&gt;
&lt;h3 id=&#34;how-to-model-sequential-data&#34;&gt;How to model sequential data?&lt;/h3&gt;
&lt;p&gt;In deep learning, the key to processing sequential data lies in &lt;strong&gt;understanding the relationship between inputs and outputs&lt;/strong&gt;. Generally, the input-output relationships of data can be divided into three categories: &lt;strong&gt;one-to-one&lt;/strong&gt;, &lt;strong&gt;one-to-many&lt;/strong&gt;, and &lt;strong&gt;many-to-many&lt;/strong&gt;. A one-to-one relationship means that each input has a corresponding output, which is commonly used in image classification. In contrast, a one-to-many relationship indicates that a single input can produce multiple outputs, suitable for generation tasks, such as a chatbot generating multiple responses based on user input. Meanwhile, a many-to-many relationship involves multiple inputs leading to multiple outputs, typically used in sequence transformation tasks, such as machine translation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RNN and NLP Study Notes 02</title>
      <link>http://localhost:1313/posts/nlp02/</link>
      <pubDate>Mon, 24 Mar 2025 11:41:20 +0800</pubDate>
      <guid>http://localhost:1313/posts/nlp02/</guid>
      <description>&lt;h2 id=&#34;text-processing-and-word-embedding&#34;&gt;Text Processing and Word Embedding&lt;/h2&gt;
&lt;h3 id=&#34;text-to-sequence&#34;&gt;Text to Sequence&lt;/h3&gt;
&lt;p&gt;Processing text data is crucial for natural language processing (NLP) and machine learning applications. This section outlines the key steps in processing text data.&lt;/p&gt;
&lt;h4 id=&#34;step-1-tokenization&#34;&gt;Step 1: Tokenization&lt;/h4&gt;
&lt;p&gt;The first step in text processing is &lt;strong&gt;tokenization&lt;/strong&gt;, which involves &lt;strong&gt;breaking down a text string into a list of individual words&lt;/strong&gt;. For example, given the text:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;S = &amp;#34;Machine learning is an important branch of artificial intelligence&amp;#34;  
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Breaking this string into a word list:&lt;/p&gt;</description>
    </item>
    <item>
      <title>RNN and NLP Study Notes 01</title>
      <link>http://localhost:1313/posts/nlp01/</link>
      <pubDate>Sun, 23 Mar 2025 21:43:18 +0800</pubDate>
      <guid>http://localhost:1313/posts/nlp01/</guid>
      <description>&lt;h2 id=&#34;data-processing-basics&#34;&gt;Data Processing Basics&lt;/h2&gt;
&lt;p&gt;In machine learning, data types can be classified into several forms, including &lt;strong&gt;numeric features&lt;/strong&gt;, &lt;strong&gt;categorical features&lt;/strong&gt;, and &lt;strong&gt;binary features&lt;/strong&gt;. The table below provides concrete examples to better understand these data types:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Age&lt;/th&gt;
          &lt;th&gt;Gender&lt;/th&gt;
          &lt;th&gt;Nationality&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;35&lt;/td&gt;
          &lt;td&gt;Male&lt;/td&gt;
          &lt;td&gt;US&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;31&lt;/td&gt;
          &lt;td&gt;Male&lt;/td&gt;
          &lt;td&gt;China&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;29&lt;/td&gt;
          &lt;td&gt;Female&lt;/td&gt;
          &lt;td&gt;India&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;27&lt;/td&gt;
          &lt;td&gt;Male&lt;/td&gt;
          &lt;td&gt;US&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;numeric-features&#34;&gt;Numeric Features&lt;/h3&gt;
&lt;p&gt;Numeric features refer to data that possess &lt;strong&gt;additive properties&lt;/strong&gt; and &lt;strong&gt;can be compared in magnitude&lt;/strong&gt;. For example, a person&amp;rsquo;s age serves as a numeric feature, where 35 is greater than 31.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Zongsi (Tristan) Xu</title>
      <link>http://localhost:1313/resume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/resume/</guid>
      <description>Personal resume of Zongsi (Tristan) Xu</description>
    </item>
  </channel>
</rss>
