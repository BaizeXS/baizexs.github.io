[{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"Making RNNs More Effective 包含三个方面：Stacked RNN、Bidirectional RNN、Pretrain\nStacked RNN 可以将很多全连接层堆叠起来，构成一个MLP；可以把很多卷积层堆叠起来，构成一个深度卷积网络；同样的道理，你可以将很多RNN堆叠起来，构成一个多层RNN网络；神经网络每一步都会更新状态h，新算出来的h有两个copies；一个送到下一个时刻，另一个作为输出，这一层的输出h成为了下一层的输入；\n最底层的RNN的输入为词向量x；这些词向量的输出h会作为下一层RNN的输入；\n![image-20250325234633209](../../../../Library/Application Support/typora-user-images/image-20250325234633209.png)\n最上层的状态为RNN的输出；最上层的最后一个状态h_t为最终的状态h_t，\nBidirectional RNN Standard RNN和人的阅读习惯一样，从左往右阅读；人阅读的过程在脑子里积累阅读的信息；RNN在状态向量h_t中积累阅读到的信息；读完一份电影评论则可以知道是正面评价还是负面评价；我们人类总是从前往后，从左往右进行阅读；但是这只是人类的阅读习惯，从后往前阅读依旧可以判断电影评价是正面的还是负面的；对RNN来说，从前往后阅读或者从后往前阅读，是没有太大区别的，训练一个从后往前阅读的RNN，也会有一个比较不错的结果；所以比较自然的想法就是训练两条RNN，一条从左往右读，一条从右往左读；两条RNN是独立的，不共享参数，也不共享状态；两条RNN各自输出自己的状态向量，然后把它们的状态向量做cat；记做向量y；如果有多层RNN，就将这些y作为更上一层RNN的输入；如果只有一层RNN，则可以将y丢弃，只保留两条链最后的状态h_t和h_t‘；把这两个向量的cat作为从输入文本中提取出的特征向量；把这个向量用于判断是正面还是负面；\n![image-20250326000815290](../../../../Library/Application Support/typora-user-images/image-20250326000815290.png)\n双向RNN总是比单向RNN效果好；原因可能是这样的，无论哪个方向，或多或少会忘记最开始的输入；从左往右阅读可能忘记最左边的内容；反之亦然；\nPretrain 预训练在深度学习中非常好用，比如在卷积神经网络中，如果网络太大而训练集不够大，那么可以在ImageNet大数据上做预训练；这样可以让神经网络有比较好的初始化，也可以避免Overfitting；训练RNN同理；例如这里Embedding层有320000参数，而我们只有2w个训练样本；这个Embedding太大了，会导致overfitting；解决办法就是对Embedding层做预训练；\n![image-20250326001446063](../../../../Library/Application Support/typora-user-images/image-20250326001446063.png)\n预训练具体如下；首先找一个更大的数据集，可以是情感分析的数据，也可以是其他类型的数据；但是任务最好是接近原来情感分析的任务；最好学习到的词向量带有正面或者负面情感；两个任务越相似，预训练之后的transfer就会越好；有了大数据集之后，要搭建一个神经网络，这个神经网络的结构是什么样都可以，甚至不用是RNN，只要这个神经网络有embedding层即可；然后就是在大数据集上训练这个神经网络；\n![image-20250326001629650](../../../../Library/Application Support/typora-user-images/image-20250326001629650.png)\n训练好之后，丢掉上面的层，只保留Embedding层和训练好的模型参数；\n![image-20250326002121770](../../../../Library/Application Support/typora-user-images/image-20250326002121770.png)\n然后再搭我们自己的RNN网络；这个新的RNN网络和之前预训练的可以有不同的结构，搭好之后，新的RNN层和全连接层都是随机初始化的，而下面的Embedding层的参数是预训练出来的，要固定住Embedding层的参数，不训练Embedding，只训练其他层；\nSummary SimpleRNN and LSTM are two kinds of RNNs; always use LSTM instead of SimpleRNN. Use Bi-RNN instead of RNN whenever possible. Stacked RNN may be better than a single RNN layer (if $n$ is big). Pretrain the embedding layer (if $n$ is small). 还有一些其他的RNN替代，例如GRU，Gated R Unit；但是不一定比LSTM好。\n","permalink":"http://localhost:1313/posts/nlp05/","summary":"\u003ch2 id=\"making-rnns-more-effective\"\u003eMaking RNNs More Effective\u003c/h2\u003e\n\u003cp\u003e包含三个方面：Stacked RNN、Bidirectional RNN、Pretrain\u003c/p\u003e\n\u003ch3 id=\"stacked-rnn\"\u003eStacked RNN\u003c/h3\u003e\n\u003cp\u003e可以将很多全连接层堆叠起来，构成一个MLP；可以把很多卷积层堆叠起来，构成一个深度卷积网络；同样的道理，你可以将很多RNN堆叠起来，构成一个多层RNN网络；神经网络每一步都会更新状态h，新算出来的h有两个copies；一个送到下一个时刻，另一个作为输出，这一层的输出h成为了下一层的输入；\u003c/p\u003e\n\u003cp\u003e最底层的RNN的输入为词向量x；这些词向量的输出h会作为下一层RNN的输入；\u003c/p\u003e\n\u003cp\u003e![image-20250325234633209](../../../../Library/Application Support/typora-user-images/image-20250325234633209.png)\u003c/p\u003e\n\u003cp\u003e最上层的状态为RNN的输出；最上层的最后一个状态h_t为最终的状态h_t，\u003c/p\u003e\n\u003ch3 id=\"bidirectional-rnn\"\u003eBidirectional RNN\u003c/h3\u003e\n\u003cp\u003eStandard RNN和人的阅读习惯一样，从左往右阅读；人阅读的过程在脑子里积累阅读的信息；RNN在状态向量h_t中积累阅读到的信息；读完一份电影评论则可以知道是正面评价还是负面评价；我们人类总是从前往后，从左往右进行阅读；但是这只是人类的阅读习惯，从后往前阅读依旧可以判断电影评价是正面的还是负面的；对RNN来说，从前往后阅读或者从后往前阅读，是没有太大区别的，训练一个从后往前阅读的RNN，也会有一个比较不错的结果；所以比较自然的想法就是训练两条RNN，一条从左往右读，一条从右往左读；两条RNN是独立的，不共享参数，也不共享状态；两条RNN各自输出自己的状态向量，然后把它们的状态向量做cat；记做向量y；如果有多层RNN，就将这些y作为更上一层RNN的输入；如果只有一层RNN，则可以将y丢弃，只保留两条链最后的状态h_t和h_t‘；把这两个向量的cat作为从输入文本中提取出的特征向量；把这个向量用于判断是正面还是负面；\u003c/p\u003e\n\u003cp\u003e![image-20250326000815290](../../../../Library/Application Support/typora-user-images/image-20250326000815290.png)\u003c/p\u003e\n\u003cp\u003e双向RNN总是比单向RNN效果好；原因可能是这样的，无论哪个方向，或多或少会忘记最开始的输入；从左往右阅读可能忘记最左边的内容；反之亦然；\u003c/p\u003e\n\u003ch3 id=\"pretrain\"\u003ePretrain\u003c/h3\u003e\n\u003cp\u003e预训练在深度学习中非常好用，比如在卷积神经网络中，如果网络太大而训练集不够大，那么可以在ImageNet大数据上做预训练；这样可以让神经网络有比较好的初始化，也可以避免Overfitting；训练RNN同理；例如这里Embedding层有320000参数，而我们只有2w个训练样本；这个Embedding太大了，会导致overfitting；解决办法就是对Embedding层做预训练；\u003c/p\u003e\n\u003cp\u003e![image-20250326001446063](../../../../Library/Application Support/typora-user-images/image-20250326001446063.png)\u003c/p\u003e\n\u003cp\u003e预训练具体如下；首先找一个更大的数据集，可以是情感分析的数据，也可以是其他类型的数据；但是任务最好是接近原来情感分析的任务；最好学习到的词向量带有正面或者负面情感；两个任务越相似，预训练之后的transfer就会越好；有了大数据集之后，要搭建一个神经网络，这个神经网络的结构是什么样都可以，甚至不用是RNN，只要这个神经网络有embedding层即可；然后就是在大数据集上训练这个神经网络；\u003c/p\u003e\n\u003cp\u003e![image-20250326001629650](../../../../Library/Application Support/typora-user-images/image-20250326001629650.png)\u003c/p\u003e\n\u003cp\u003e训练好之后，丢掉上面的层，只保留Embedding层和训练好的模型参数；\u003c/p\u003e\n\u003cp\u003e![image-20250326002121770](../../../../Library/Application Support/typora-user-images/image-20250326002121770.png)\u003c/p\u003e\n\u003cp\u003e然后再搭我们自己的RNN网络；这个新的RNN网络和之前预训练的可以有不同的结构，搭好之后，新的RNN层和全连接层都是随机初始化的，而下面的Embedding层的参数是预训练出来的，要固定住Embedding层的参数，不训练Embedding，只训练其他层；\u003c/p\u003e\n\u003ch3 id=\"summary\"\u003eSummary\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSimpleRNN and LSTM are two kinds of RNNs; always use LSTM instead of SimpleRNN.\u003c/li\u003e\n\u003cli\u003eUse Bi-RNN instead of RNN whenever possible.\u003c/li\u003e\n\u003cli\u003eStacked RNN may be better than a single RNN layer (if $n$ is big).\u003c/li\u003e\n\u003cli\u003ePretrain the embedding layer (if $n$ is small).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e还有一些其他的RNN替代，例如GRU，Gated R Unit；但是不一定比LSTM好。\u003c/p\u003e","title":"RNN and NLP Study Notes 05"},{"content":"Long Short Term Memory (LSTM) LSTM Model Long Short-Term Memory networks (LSTM) are a specialized type of recurrent neural network (RNN) designed to effectively handle dependencies in long sequence data. Traditional RNNs often face the problem of forgetting when processing long time sequences, making it difficult to retain important information. LSTM addresses this issue by introducing a \u0026ldquo;cell state\u0026rdquo; mechanism that allows information to flow directly across time steps, helping the network better capture long-term dependencies.\n![image-20250326144600920](../../../../Library/Application Support/typora-user-images/image-20250326144600920.png)\nThe structure of LSTM is more complex than that of a standard simple RNN. Each of its repeating modules consists of multiple interacting neural network layers rather than just a single layer. Specifically, LSTM uses four parameter matrices to control the forgetting, updating, and output of information, which enhances its capability in modeling sequence data. The design of LSTM\u0026rsquo;s input gate, forget gate, and output gate ensures that it can actively select which information should be retained or updated, allowing it to excel in processing complex sequence tasks. This flexibility makes LSTM widely used in fields such as natural language processing and speech recognition, helping to solve many practical problems.\nStep-by-Step LSTM Walk Through The core of LSTM lies in its \u0026ldquo;cell state,\u0026rdquo; which is a horizontal line that runs through the entire network. The cell state can be thought of as a conveyor belt that flows directly through the network while engaging in only a small amount of linear interaction. This allows information to flow easily and remain unchanged along this conveyor belt.\n![image-20250326145155175](../../../../Library/Application Support/typora-user-images/image-20250326145155175.png)\nThe \u0026ldquo;gates\u0026rdquo; in LSTM are mechanisms that can selectively allow information to pass through. Each gate consists of a sigmoid neural network layer and a pointwise multiplication operation. The sigmoid layer outputs a value between 0 and 1, determining how much of each piece of information should be allowed to pass. A value of 0 means \u0026ldquo;no information is allowed to pass,\u0026rdquo; while a value of 1 means \u0026ldquo;all information is allowed to pass.”\n![image-20250326145807368](../../../../Library/Application Support/typora-user-images/image-20250326145807368.png)\nLSTM has three such gates, responsible for protecting and controlling the cell state, ensuring that information can flow and update effectively within the network.\nForget Gate The forget gate $f_t$ in Long Short-Term Memory (LSTM) networks is responsible for determining which information from the cell state should be discarded, consisting mainly of a sigmoid function and elementwise multiplication.\n![image-20250326151710194](../../../../Library/Application Support/typora-user-images/image-20250326151710194.png)\nFirst, the forget gate concatenates the previous hidden state vector $h$ with the current input vector $x$ to form a new vector. Then, it applies the sigmoid function to compress each element of this concatenated vector to a value between 0 and 1, determining the extent to which each piece of information should be retained.\n![image-20250326151436309](../../../../Library/Application Support/typora-user-images/image-20250326151436309.png)\nSubsequently, each element of the output vector $f_t$ generated by the forget gate is multiplied elementwise with the corresponding element of the cell state $c$. This means that if any element of $f_t$ is 0, the corresponding element of $c$ will be completely forgotten, resulting in an output of 0; conversely, if an element of $f_t$ is 1, the corresponding element of $c$ can fully pass through, maintaining its value. Therefore, the design of the forget gate allows the LSTM to flexibly choose which information to retain and which to discard.\n![image-20250326151451949](../../../../Library/Application Support/typora-user-images/image-20250326151451949.png)\nMathematically, the calculation of the forget gate is represented as: $$ f(t) = W_f \\cdot \\sigma([h_{t-1},x_t] + b_f) $$ Here, $W_f$ is the weight matrix of the forget gate, $\\sigma$ is the sigmoid activation function, $[h_{t-1}, x_t]$ represents the concatenation of the previous hidden state $h_{t-1}$ and the current input $x_t$, and $b_f$ is the bias term. In this way, the forget gate can effectively control the flow of information, ensuring the effective learning and stable operation of the LSTM network.\nInput Gate The input gate $i_t$ in Long Short-Term Memory (LSTM) networks decides which values from the conveyor belt will be updated. It relies on the previous hidden state $h_{t-1}$ and the current input $x_t$. The first step of the input gate is similar to that of the forget gate, where it concatenates $h_{t-1}$ and $x_t$, then applies the sigmoid function to obtain the output $i_t$, which ranges between (0, 1).\n![image-20250326161507245](../../../../Library/Application Support/typora-user-images/image-20250326161507245.png)\nMathematically, the calculation of the input gate is represented as: $$ i(t) = W_i \\cdot \\sigma([h_{t-1}, x_t] + b_i) $$ Here, $W_i$ is the weight matrix for the input gate, $\\sigma$ is the sigmoid activation function, and $b_i$ is the bias term.\nCandidate Memory Cell In addition to the input gate, we need to compute the new candidate value $\\tilde{c}_t$, which will be added to the conveyor belt. This computation is similar to that of $i_t$ but uses the $\\tanh$ function instead of the sigmoid function. Thus, the elements of $\\tilde{c}_t$ range between $(-1, 1)$.\n![image-20250326161749809](../../../../Library/Application Support/typora-user-images/image-20250326161749809.png)\nMathematically, the calculation of the candidate memory cell is represented as: $$ \\tilde{c}(t) = W_c \\cdot \\tanh([h_{t-1}, x_t] + b_c) $$ Here, $W_c$ is the weight matrix for the candidate memory cell, $\\tanh$ is the hyperbolic tangent activation function, and $b_c$ is the bias term.\nUpdating the Cell State With the known values of the previous cell state $C_{t-1}$, the output of the forget gate $f_t$, the output of the input gate $i_t$, and the candidate value $\\tilde{c}_t$, we can now proceed to update the cell state $C$.\nThe forget gate $f_t$ is multiplied elementwise with the old cell state $C_{t-1}$ to determine which elements to forget. If any element of $f_t$ is 0, the corresponding element of $C_{t-1}$ will be completely forgotten.\n![image-20250326162118762](../../../../Library/Application Support/typora-user-images/image-20250326162118762.png)\nNext, after selectively forgetting some elements in the cell state, we need to add new information to the conveyor belt. This is done by performing elementwise multiplication between the input gate output $i_t$ and the candidate value $\\tilde{c}_t$, followed by adding these two results together to produce the new cell state $C_t$.\n![image-20250326162147608](../../../../Library/Application Support/typora-user-images/image-20250326162147608.png)\nMathematically, the update of the cell state is represented as: $$ C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{c}_t $$ Where $\\odot$ denotes elementwise multiplication.\nThis completes one update cycle of the conveyor belt. After updating the cell state $C$, we need to compute the LSTM\u0026rsquo;s output $h_t$. First, we calculate the output gate $o_t$, which follows a similar procedure to that of the input gate and the forget gate.\nOutput Gate The output gate $o_t$ in Long Short-Term Memory (LSTM) networks decides what information flows from the conveyor belt $C_t$ to the hidden state $h_t$. It takes into account the previous hidden state $h_{t-1}$ and the current input $x_t$.\nThe first step involves concatenating $h_{t-1}$ and $x_t$, after which the concatenated vector is processed through a sigmoid activation function to produce $o_t$. The output $o_t$ ranges between $(0, 1)$, indicating the extent to which information should be kept from the cell state.\n![image-20250326163741544](../../../../Library/Application Support/typora-user-images/image-20250326163741544.png)\nMathematically, the calculation of the output gate is represented as: $$ o(t) = W_o \\cdot \\sigma([h_{t-1}, x_t] + b_o) $$ Here, $W_o$ is the weight matrix for the output gate, $\\sigma$ is the sigmoid activation function, and $b_o$ is the bias term.\nCalculating the Output State To compute the output state $h_t$, we first apply the hyperbolic tangent function ($\\tanh$) to the current cell state $C_t$. This compresses each element to a range of $(-1, 1)$. Then, we multiply the result elementwise by the output gate $o_t$.\n![image-20250326164043978](../../../../Library/Application Support/typora-user-images/image-20250326164043978.png)\nMathematically, this is expressed as: $$ h_t = o_t \\odot \\tanh(C_t) $$ Here, $o_t$ provides the gating mechanism that controls which parts of $C_t$ are passed to $h_t$ through elementwise multiplication $\\odot$.\nState Representation The output state $h_t$ serves as the final output of the LSTM. It has two copies: one forwarded to the next time step and the other used as the current output. By the time step $t$, all input vectors $x$ processed by the LSTM contribute to the information in $h_t$, enabling the network to effectively utilize historical context for its predictions.\nThis structure ensures that the LSTM can maintain relevant information across time steps while controlling what is outputted in each cycle.\nUnderstanding $C_t$ and $h_t$ in LSTM In Long Short-Term Memory (LSTM) networks, $C_t$ and $h_t$ are two critical state variables, each serving distinct purposes:\n$C_t$ (Cell State): Represents long-term memory within the LSTM structure. Used to carry information that helps the network remember long-term dependencies when processing sequential data. It is updated by retaining important information and forgetting unnecessary details, typically controlled by the forget gate and input gate. $C_t$ is a single vector that is continuously passed along through the time steps. $h_t$ (Hidden State): Represents short-term memory within the LSTM structure. Captures the output of the current time step, which can be used for predicting the next time step\u0026rsquo;s input. $h_t$ is derived from the cell state after passing through a nonlinear transformation (usually the $\\tanh$ function) and is sent to the network\u0026rsquo;s output layer. It can be viewed as a summary of the information at the current time step, serving as the output of the network at that moment. In summary, $C_t$ serves to store long-term memories, allowing the network to track information more meticulously, while $h_t$ is a short-term representation of that state, emphasizing the output at the current time step. Together, these states enable LSTMs to effectively process time series data and learn long-range dependencies.\nModel Parameters of LSTM When discussing the parameters of LSTM, attention should be paid to the dimensions of each parameter matrix. Specifically, there are a total of four parameter matrices in LSTM. The number of rows in each parameter matrix corresponds to the dimension of the hidden state $shape(h)$, while the number of columns is the sum of the hidden state dimension and the input dimension $shape(h) + shape(x)$. Therefore, the total number of parameters in these matrices can be represented as:\n#parameter matrices: 4 #rows in each matrix: $shape(h)$ #columns in each matrix: $shape(h) + shape(x)$ Total #parameters (not counting intercept): $4 \\times shape(h) \\times [shape(h) + shape(x)]$ This means that as the input scale increases, the number of parameters in LSTM will also grow accordingly, thereby influencing the model\u0026rsquo;s complexity and training requirements.\nSummary LSTM uses a “conveyor belt” to get longer memory than SimpleRNN.\nEach of the following blocks has a parameter matrix:\nForget gate. Input gate. New values. Output gate. Number of parameters: $$ 4 \\times shape(h) \\times [shape(h) + shape(x)] $$\ndropout也可以用在LSTM上\n","permalink":"http://localhost:1313/posts/nlp04/","summary":"\u003ch2 id=\"long-short-term-memory-lstm\"\u003eLong Short Term Memory (LSTM)\u003c/h2\u003e\n\u003ch3 id=\"lstm-model\"\u003eLSTM Model\u003c/h3\u003e\n\u003cp\u003eLong Short-Term Memory networks (LSTM) are a specialized type of recurrent neural network (RNN) designed to effectively handle dependencies in long sequence data. Traditional RNNs often face the problem of forgetting when processing long time sequences, making it difficult to retain important information. LSTM addresses this issue by introducing a \u0026ldquo;cell state\u0026rdquo; mechanism that allows information to flow directly across time steps, helping the network better capture long-term dependencies.\u003c/p\u003e","title":"RNN and NLP Study Notes 04"},{"content":"Recurrent Neural Networks (RNNs) How to model sequential data? In deep learning, the key to processing sequential data lies in understanding the relationship between inputs and outputs. Generally, the input-output relationships of data can be divided into three categories: one-to-one, one-to-many, and many-to-many. A one-to-one relationship means that each input has a corresponding output, which is commonly used in image classification. In contrast, a one-to-many relationship indicates that a single input can produce multiple outputs, suitable for generation tasks, such as a chatbot generating multiple responses based on user input. Meanwhile, a many-to-many relationship involves multiple inputs leading to multiple outputs, typically used in sequence transformation tasks, such as machine translation.\nOne to One: Definition: Each input corresponds to one output. Application Example: Commonly used in image classification tasks, where the input is an image and the output is the label of that image. One to Many: Definition: One input can generate multiple outputs. Application Example: In a chatbot, multiple possible responses can be generated after inputting a question from the user, or multiple descriptions can be generated for an image. Many to Many: Definition: Multiple inputs can produce multiple outputs, and the lengths of the sequences are usually different. Application Example: In machine translation tasks, multiple words from the input sentence correspond to multiple words in the translated output. Fully connected networks (FC Nets) and convolutional neural networks (ConvNets) perform well when handling fixed-size inputs and outputs, but they struggle with variable-length sequential data. Since these two models typically only allow for one-to-one or one-to-many mappings, they cannot capture the temporal dependencies and contextual information between inputs.\nLimitations of FC Nets and ConvNets:\nProcess a paragraph as a whole. Fixed-size input (e.g., image). Fixed-size output (e.g., predicted probabilities). In comparison, recurrent neural networks (RNNs) are more suitable for modeling sequential data. RNNs can handle variable-length inputs and outputs while continuously preserving contextual information through hidden states, which enables them to demonstrate greater flexibility and capability in tasks such as sequential generation and time series analysis. RNNs are better ways to model sequential data (e.g., text, speech, and time series).\nRecurrent Neural Networks (RNNs) are designed to handle sequential data, where the relationships between inputs and outputs are often many-to-one or many-to-many rather than one-to-one. In an RNN, each word in a sequence contributes to the accumulation of information in the state vector $h_t$. Initially, each input word is transformed into a word embedding, producing a corresponding vector $x_t$. As each word vector is fed into the RNN, the network updates the state $h$ to incorporate the new information. For instance, $h_0$ encapsulates the information from the first word $x_0$, while $h_1$ contains information from the first two words $x_0$ and $x_1$, and so on. By the time the last word is processed, the final state $h_t$ represents a feature vector that summarizes the entire sentence. The state updates rely on a parameter matrix $A$, which remains constant throughout the RNN, regardless of the length of the sequence being processed. This design allows RNNs to effectively capture dependencies across sequences and utilize contextual information.\nSimple RNN Model Recurrent Neural Networks (RNN) Models and Working Principles Recurrent Neural Networks (RNNs) are a type of neural network model specifically designed for handling sequential data. Their structure allows the state at each moment to depend not only on the current input but also on the previous state, creating a dynamic memory mechanism. In an RNN, the two inputs to the model are the previous state $h_{t-1}$ and the current input word vector $x_t$.\nDuring the state update, the RNN first concatenates $h_{t-1}$ and $x_t$ to generate a higher-dimensional vector. This vector is then multiplied by the model\u0026rsquo;s parameter matrix $A$ to obtain a new vector. This new vector is processed through an activation function, which is typically the hyperbolic tangent function $\\tanh$, serving to compress each element\u0026rsquo;s value within the range of -1 to 1. In this way, the state $h_t$ is effectively updated, allowing us to interpret the new state $h_t$ as a function of the current input $x_t$ and the previous state $h_{t-1}$.\nThe computation of state $h_t$ relies on three factors:\nthe current input $x_t$, the previous state $h_{t-1}$, and the parameter matrix $A$. The parameter matrix $A$ plays a crucial role in the calculations of the entire RNN, driving the process of each state update.\nChoosing the Activation Function: Why Use the Tanh Function? The activation function plays a key role in deep learning models. Without an appropriate activation function, the model may encounter issues of gradient vanishing or explosion. The function of tanh is to perform \u0026ldquo;normalization\u0026rdquo; after each weight update. By readjusting the values to a reasonable range of -1 to 1, the tanh function helps maintain the stability of the model and enhances training efficiency. This normalization process ensures that the model can learn and update the weights of each layer more accurately, thereby improving overall performance.\nModel Parameters of Simple RNN When discussing the parameters of RNNs, attention should be paid to the dimensions of the parameter matrix $A$. Specifically, the number of rows in matrix $A$ corresponds to the dimension of the hidden state $shape(h)$, while the number of columns is the sum of the hidden state dimension and the input dimension ($shape(h) + shape(x)$). Therefore, the total number of parameters in this matrix can be represented as:\n#rows of $A$: $shape(h)$ #cols of $A$: $shape(h) + shape(x)$ Total #parameter: $shape(h) \\times [shape(h) + shape(x)]$. This means that as the input scale increases, the number of parameters will also grow, thereby influencing the model\u0026rsquo;s complexity and training requirements.\nLimitations of Simple RNN Although Simple RNNs perform well in handling short-term dependencies, they exhibit significant shortcomings in addressing long-term dependencies. This is because, in RNNs, the current state $h$ is functionally related to all previous states $h$. Theoretically, if the early input $x_1$ is changed, it should lead to changes in all subsequent states $h$. However, in practice, Simple RNNs do not fully demonstrate this property. By taking the derivative of state $h_{100}$ with respect to $x_1$, it can be observed that the derivative approaches zero, indicating that when $x_1$ changes, $h_{100}$ hardly changes at all. This shows that state $h_{100}$ has almost no relation to earlier inputs $x_0$, meaning that state $h_{100}$ has forgotten information from many previous steps, highlighting the limitations of RNNs in handling long-term dependency issues.\nThese designs and limitations point to the use cases of Simple RNNs as well as the directions for future model improvements, leading to the development of more complex RNN variants such as Long Short-Term Memory networks (LSTMs) and Gated Recurrent Units (GRUs), aimed at addressing issues of long-term dependencies.\nSummary RNN for text, speech, and time series data.\nHidden state $h_t$ aggregates information in the inputs $x_0, \\dots, x_t$.\nRNNs can forget early inputs.\nIt forgets what it has seen early on. If $t$ is large, $h_t$ is almost irrelevant to $x_0$. SimpleRNN has a parameter matrix (and perhaps an intercept vector).\nShape of the parameter matrix is\n$$ shape(h) \\times [shape(h) + shape(x)]. $$\nOnly one such parameter matrix, no matter how long the sequence is.\n","permalink":"http://localhost:1313/posts/nlp03/","summary":"\u003ch2 id=\"recurrent-neural-networks-rnns\"\u003eRecurrent Neural Networks (RNNs)\u003c/h2\u003e\n\u003ch3 id=\"how-to-model-sequential-data\"\u003eHow to model sequential data?\u003c/h3\u003e\n\u003cp\u003eIn deep learning, the key to processing sequential data lies in \u003cstrong\u003eunderstanding the relationship between inputs and outputs\u003c/strong\u003e. Generally, the input-output relationships of data can be divided into three categories: \u003cstrong\u003eone-to-one\u003c/strong\u003e, \u003cstrong\u003eone-to-many\u003c/strong\u003e, and \u003cstrong\u003emany-to-many\u003c/strong\u003e. A one-to-one relationship means that each input has a corresponding output, which is commonly used in image classification. In contrast, a one-to-many relationship indicates that a single input can produce multiple outputs, suitable for generation tasks, such as a chatbot generating multiple responses based on user input. Meanwhile, a many-to-many relationship involves multiple inputs leading to multiple outputs, typically used in sequence transformation tasks, such as machine translation.\u003c/p\u003e","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding Text to Sequence Processing text data is crucial for natural language processing (NLP) and machine learning applications. This section outlines the key steps in processing text data.\nStep 1: Tokenization The first step in text processing is tokenization, which involves breaking down a text string into a list of individual words. For example, given the text:\n1 S = \u0026#34;Machine learning is an important branch of artificial intelligence\u0026#34; Breaking this string into a word list:\n1 L = [\u0026#34;machine\u0026#34;, \u0026#34;learning\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;an\u0026#34;, \u0026#34;important\u0026#34;, \u0026#34;branch\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;artificial\u0026#34;, \u0026#34;intelligence\u0026#34;] This shows the simplest form of tokenization, but in reality, many factors need to be considered, such as:\nConverting upper case to lower case (e.g., changing \u0026ldquo;Apple\u0026rdquo; to \u0026ldquo;apple\u0026rdquo;). Removing stop words, such as \u0026ldquo;the,\u0026rdquo; \u0026ldquo;a,\u0026rdquo; \u0026ldquo;of,\u0026rdquo; etc. Correcting typos (e.g., changing \u0026ldquo;goood\u0026rdquo; to \u0026ldquo;good\u0026rdquo;). Nowadays, commonly used tokenization methods include BPE (Byte Pair Encoding), WordPiece, and SentencePiece.\nStep 2: Build Dictionary After obtaining the word list, the next step is to build a dictionary that maps each word to a unique index and counts the frequency of each word. This can be accomplished using a dictionary (hash table) that records each word along with its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If $w$ is not in the dictionary, add $w$ and set its frequency to 1. If $w$ is already in the dictionary, increment its frequency counter. After processing some text, the dictionary might look like this:\nWord Frequency machine learning 219 artificial intelligence 200 deep learning 180 model 131 algorithm 120 training 52 prediction 31 Step 3: One-Hot Encoding Using a dictionary to map words to indices\nUtilizing the previously constructed dictionary, map each word in the text to its corresponding index (integer). Then, these indices form a sequence. For example, suppose the dictionary contains the following entries derived from the frequency table:\nWord Index machine learning 1 artificial intelligence 2 deep learning 3 model 4 algorithm 5 training 6 prediction 7 an 8 important 9 branch 10 of 11 intelligence 12 is 13 The corresponding sequence for the sentence would be:\n1 sequences = [1, 13, 8, 9, 10, 11, 2] Generating one-hot vectors\nFor each word in the sequence, a one-hot vector is generated based on its index. A one-hot vector is a $v$-dimensional vector, where $v$ is the size of the vocabulary.\nGiven the vocabulary size of 13 (assuming distinct words from the example), each word\u0026rsquo;s one-hot vector will be structured as follows:\n1 2 3 4 5 6 7 \u0026#34;machine learning\u0026#34; = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \u0026#34;artificial intelligence\u0026#34; = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \u0026#34;is\u0026#34; = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] \u0026#34;an\u0026#34; = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] \u0026#34;important\u0026#34; = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] \u0026#34;branch\u0026#34; = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] \u0026#34;of\u0026#34; = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] For the example sentence \u0026ldquo;Machine learning is an important branch of artificial intelligence\u0026rdquo;, the corresponding one-hot encoding results would be:\n1 2 3 4 5 6 7 8 9 [ [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], // \u0026#34;machine learning\u0026#34; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], // \u0026#34;is\u0026#34; [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], // \u0026#34;an\u0026#34; [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], // \u0026#34;important\u0026#34; [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], // \u0026#34;branch\u0026#34; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], // \u0026#34;of\u0026#34; [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], // \u0026#34;artificial intelligence\u0026#34; ] In this way, all words in the example sentence are transformed into corresponding one-hot encodings, which can be used for subsequent machine learning tasks.\nStep 4: Align Sequences In the process of text processing, Aligning Sequences is a crucial step to ensure that all input samples have the same length. After the previous processing steps, we found that there are differences in the lengths of the sequences, which poses a challenge for building machine learning models. Most machine learning models require uniform input shapes, so we must perform sequence alignment.\nTo address this issue, the following measures can be implemented:\nFixed Length: Choose a fixed length $w$. Truncation: If a sequence exceeds the length $w$, retain only the last $w$ words. Padding: If a sequence is shorter than $w$, pad it with zeros until it reaches the specified length $w$. By doing this, all sequences will be adjusted to the same length, allowing them to be effectively stored in a matrix and facilitating subsequent training of machine learning models. This process will help improve the model\u0026rsquo;s performance and accuracy.\nWord Embedding: Word to Vector Why map words to vectors? The main reasons for mapping words to vectors in natural language processing (NLP) include:\nCapturing semantic relationships\nTraditional word representations (like one-hot vectors) only indicate the presence or absence of words, lacking the ability to express similarities between words. By mapping to a low-dimensional vector space, word embeddings can capture semantic similarities between words. For example, \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; would be closer in vector space, while \u0026ldquo;king\u0026rdquo; and \u0026ldquo;apple\u0026rdquo; would be farther apart.\nDimensionality reduction\nOne-hot encoding generates a sparse vector with a dimension equal to the size of the vocabulary, leading to significant storage and computational costs. Word embeddings represent words as low-dimensional dense vectors, significantly reducing the required storage space and computation.\nImproving model performance\nUsing low-dimensional embedding vectors allows machine learning models to learn and predict more efficiently. Embedding vectors enable models to better understand and process complex patterns in language, thus enhancing performance in tasks like sentiment analysis and text classification.\nFacilitating transfer learning\nPre-trained word embedding vectors can be transferred between different tasks and datasets, accelerating model training and improving prediction accuracy. This makes word embeddings widely used in various NLP tasks.\nHow to map word to vector? Represent words using one-hot vectors\nFirst, represent words using one-hot vectors.\nAssume the dictionary contains $v$ unique words (vocabulary = $v$). Then the one-hot vectors $e_1$, $e_2$, $e_3$, …, $e_v$ are $v$-dimensional. Map one-hot vectors to low-dimensional vectors\nNext, map the one-hot vectors to low-dimensional vectors. The mapping formula is: $$ x_i = P^T \\cdot e_i $$\n$x_i$ is the low-dimensional vector, with dimensions $d \\times 1$. $P$ is parameter matrix which can be learned from training data, with dimensions $d \\times v$. $e_i$ is the one-hot vector of the 𝑖-th word in dictionary, with dimensions $v \\times 1$ Interpretation of the parameter matrix The parameter matrix $P$ contains the embedding representations of each word in the low-dimensional space. Each row corresponds to a word.\nBy visualizing, you can see the relative positions of different words in low-dimensional space. Similar words are close to each other in vector space, reflecting their semantic similarity.\nFor example, the position of the word \u0026ldquo;fantastic\u0026rdquo; is close to \u0026ldquo;good,\u0026rdquo; \u0026ldquo;fun,\u0026rdquo; etc., while \u0026ldquo;boring\u0026rdquo; and \u0026ldquo;poor\u0026rdquo; are relatively far apart.\nConclusion Text processing converts raw text into structured formats suitable for analysis and consists of several key steps:\nTokenization: Breaking down text into individual words. Building a Dictionary: Mapping each word to a unique index and counting frequencies. One-Hot Encoding: Converting words to one-hot vectors. Aligning Sequences: Ensuring uniform input lengths for machine learning models. Mastering these steps is essential for efficient feature extraction and improved model performance in natural language processing tasks.\nWord embeddings transform words into low-dimensional vectors, capturing semantic relationships and enhancing expressiveness for various natural language processing tasks. The process involves:\nOne-Hot Encoding: Representing words as sparse vectors. Mapping to Low-Dimensional Vectors: Using a parameter matrix to project one-hot vectors into a reduced space. Understanding how to map words to vectors and interpret the parameter matrix is crucial for developing effective NLP applications.\nReferences For a deeper understanding of these concepts, you can refer to the following course video:RNN模型与NLP应用(2/9)：文本处理与词嵌入.\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003eProcessing text data is crucial for natural language processing (NLP) and machine learning applications. This section outlines the key steps in processing text data.\u003c/p\u003e\n\u003ch4 id=\"step-1-tokenization\"\u003eStep 1: Tokenization\u003c/h4\u003e\n\u003cp\u003eThe first step in text processing is \u003cstrong\u003etokenization\u003c/strong\u003e, which involves \u003cstrong\u003ebreaking down a text string into a list of individual words\u003c/strong\u003e. For example, given the text:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eS = \u0026#34;Machine learning is an important branch of artificial intelligence\u0026#34;  \n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eBreaking this string into a word list:\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nReferences For a deeper understanding of these concepts, you can refer to the following course video:RNN模型与NLP应用(1/9)：数据处理基础.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"}]