[{"content":"👋 About me I\u0026rsquo;m \u0026hellip;\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"👋 About me I\u0026rsquo;m \u0026hellip;\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"👋 About me I\u0026rsquo;m \u0026hellip;\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"👋 About me I\u0026rsquo;m \u0026hellip;\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"👋 About me I\u0026rsquo;m \u0026hellip;\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"👋 About me I\u0026rsquo;m \u0026hellip;\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"👋 About me I\u0026rsquo;m \u0026hellip;\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"👋 About me I\u0026rsquo;m \u0026hellip;\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"👋 About me I\u0026rsquo;m \u0026hellip;\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"👋 About me I\u0026rsquo;m \u0026hellip;\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me I\u0026rsquo;m \u0026hellip;\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me I\u0026rsquo;m \u0026hellip;\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, and computer vision.\nI have a strong interest in developing LLM agents and knowledge graphs, leveraging data science techniques for innovative solutions. Additionally, I am skilled in software development, aiming to create impactful applications that enhance user experiences.\nThroughout my academic journey, I have been involved in several innovative projects focused on fraud detection, intelligent text processing, and quantum computing applications in deep learning. My programming skills span languages like Python and Java, with experience in frameworks such as PyTorch.\nWhen I’m not coding, you can find me exploring new technologies, sharing insights on my blog, or engaging in community activities related to cyber security.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nI have a strong interest in developing LLM agents and knowledge graphs, leveraging data science techniques for innovative solutions. Additionally, I am skilled in software development, aiming to create impactful applications that enhance user experiences.\nThroughout my academic journey, I have been involved in several innovative projects focused on fraud detection, intelligent text processing, and quantum computing applications in deep learning. My programming skills span languages like Python and Java, with experience in frameworks such as PyTorch.\nWhen I’m not coding, you can find me exploring new technologies, sharing insights on my blog, or engaging in community activities related to cyber security.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in Electronic Commerce \u0026amp; Internet Computing at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nHello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nI have a strong interest in developing LLM agents and knowledge graphs, leveraging data science techniques for innovative solutions. Additionally, I am skilled in software development, aiming to create impactful applications that enhance user experiences.\nThroughout my academic journey, I have been involved in several innovative projects focused on fraud detection, intelligent text processing, and quantum computing applications in deep learning. My programming skills span languages like Python and Java, with experience in frameworks such as PyTorch.\nWhen I’m not coding, you can find me exploring new technologies, sharing insights on my blog, or engaging in community activities related to cyber security.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in Electronic Commerce \u0026amp; Internet Computing at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, I have developed a strong interest in LLM agents, reinforcement learning, and knowledge graphs, particularly inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence. I aspire to contribute to the advancement of AGI and leverage data science techniques to develop innovative solutions.\nHello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nI have a strong interest in developing LLM agents and knowledge graphs, leveraging data science techniques for innovative solutions. Additionally, I am skilled in software development, aiming to create impactful applications that enhance user experiences.\nThroughout my academic journey, I have been involved in several innovative projects focused on fraud detection, intelligent text processing, and quantum computing applications in deep learning. My programming skills span languages like Python and Java, with experience in frameworks such as PyTorch.\nWhen I’m not coding, you can find me exploring new technologies, sharing insights on my blog, or engaging in community activities related to cyber security.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in Electronic Commerce \u0026amp; Internet Computing at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, I have developed a strong interest in LLM agents, reinforcement learning, and knowledge graphs, particularly inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence.\nI aspire to contribute to the advancement of AGI by utilizing a diverse set of skills and techniques to develop innovative solutions.\nHello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nI have a strong interest in developing LLM agents and knowledge graphs, leveraging data science techniques for innovative solutions. Additionally, I am skilled in software development, aiming to create impactful applications that enhance user experiences.\nThroughout my academic journey, I have been involved in several innovative projects focused on fraud detection, intelligent text processing, and quantum computing applications in deep learning. My programming skills span languages like Python and Java, with experience in frameworks such as PyTorch.\nWhen I’m not coding, you can find me exploring new technologies, sharing insights on my blog, or engaging in community activities related to cyber security.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in Electronic Commerce \u0026amp; Internet Computing at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, multimodal large models, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\nI aspire to contribute to the advancement of AGI by utilizing a diverse set of skills and techniques to develop innovative solutions.\nHello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nI have a strong interest in developing LLM agents and knowledge graphs, leveraging data science techniques for innovative solutions. Additionally, I am skilled in software development, aiming to create impactful applications that enhance user experiences.\nThroughout my academic journey, I have been involved in several innovative projects focused on fraud detection, intelligent text processing, and quantum computing applications in deep learning. My programming skills span languages like Python and Java, with experience in frameworks such as PyTorch.\nWhen I’m not coding, you can find me exploring new technologies, sharing insights on my blog, or engaging in community activities related to cyber security.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in Electronic Commerce \u0026amp; Internet Computing at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\nI aspire to contribute to the advancement of AGI by utilizing a diverse set of skills and techniques to develop innovative solutions.\nHello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nI have a strong interest in developing LLM agents and knowledge graphs, leveraging data science techniques for innovative solutions. Additionally, I am skilled in software development, aiming to create impactful applications that enhance user experiences.\nThroughout my academic journey, I have been involved in several innovative projects focused on fraud detection, intelligent text processing, and quantum computing applications in deep learning. My programming skills span languages like Python and Java, with experience in frameworks such as PyTorch.\nWhen I’m not coding, you can find me exploring new technologies, sharing insights on my blog, or engaging in community activities related to cyber security.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in Electronic Commerce \u0026amp; Internet Computing at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\nI aspire to contribute to the advancement of AGI by utilizing a diverse set of skills and techniques to develop innovative solutions.\nHello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nI have a strong interest in developing LLM agents and knowledge graphs, leveraging data science techniques for innovative solutions. Additionally, I am skilled in software development, aiming to create impactful applications that enhance user experiences.\nThroughout my academic journey, I have been involved in several innovative projects focused on fraud detection, intelligent text processing, and quantum computing applications in deep learning. My programming skills span languages like Python and Java, with experience in frameworks such as PyTorch.\nWhen I’m not coding, you can find me exploring new technologies, sharing insights on my blog, or engaging in community activities related to cyber security.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in Electronic Commerce \u0026amp; Internet Computing at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\nThroughout my academic journey, I have been involved in several innovative projects focused on fraud detection, intelligent text processing, and quantum computing applications in deep learning. My programming skills span languages like Python and Java, with experience in frameworks such as PyTorch.\nWhen I’m not coding, you can find me exploring new technologies, sharing insights on my blog, or engaging in community activities related to cyber security.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in Electronic Commerce \u0026amp; Internet Computing at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in Electronic Commerce \u0026amp; Internet Computing at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I’m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Recently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Recently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Recently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Recently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in emerging areas of artificial intelligence, including machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nRecently, inspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I have great expectations for AGI and developed a strong interest in LLM agents, MLLM, reinforcement learning, and knowledge graphs. I believe there is significant potential in these fields to create innovative solutions, and I hope to contribute my efforts towards that goal.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI’s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI’s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 Zongsi (Tristan) Xu 💻 github.com/BaizeXS 📍 Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: github.com/BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"一、数值特征与类别特征 数值特征 (Numeric Features) 具有可加减的意义，且存在大小顺序。 例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。 类别特征 (Categorical Features) 数据值离散，不具备“数值大小”间的可比意义。 例子：国籍（美国/中国/印度等）、性别（男/女）等。 问题：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。 二、One-Hot编码的动机与流程 为什么使用One-Hot\n通过将类别映射为向量，避免整数编码引入的伪数值关系。 同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。 建立映射：从类别到索引\n为每个类别分配一个唯一索引，通常从1开始。 预留索引0表示“未知”或“缺失”，对应全0向量。 转换为One-Hot向量\n若类别总数为197，则用197维向量表示。 例子：美国 [1, 0, 0, 0, …]， 中国 [0, 1, 0, 0, …]。 示例：表示一个人的特征\n特征包含（年龄, 性别, 国籍，其中国籍197种） 维度总和：1维_age + 1维_gender + 197维_nationality = 199维。 例如 (28, 女, 中国) → [28, 0, 0, 1, 0, …, 0]。 三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 ","permalink":"http://localhost:1313/posts/nlp-1/","summary":"\u003ch2 id=\"一数值特征与类别特征\"\u003e一、数值特征与类别特征\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e数值特征 (Numeric Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e具有可加减的意义，且存在大小顺序。\u003c/li\u003e\n\u003cli\u003e例子：年龄、温度等。可以直接保留其整数或浮点数形式供模型使用。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e类别特征 (Categorical Features)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e数据值离散，不具备“数值大小”间的可比意义。\u003c/li\u003e\n\u003cli\u003e例子：国籍（美国/中国/印度等）、性别（男/女）等。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：若仅用整数表示（美国=1, 中国=2），会引入错误的数值运算关系，如 “美国 + 中国 = 印度”。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"二one-hot编码的动机与流程\"\u003e二、One-Hot编码的动机与流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e为什么使用One-Hot\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e通过将类别映射为向量，避免整数编码引入的伪数值关系。\u003c/li\u003e\n\u003cli\u003e同时保留各种可能类别的信息，且仅在对应位置为1，其他位置为0。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e建立映射：从类别到索引\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e为每个类别分配一个唯一索引，通常从1开始。\u003c/li\u003e\n\u003cli\u003e预留索引0表示“未知”或“缺失”，对应全0向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e转换为One-Hot向量\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e若类别总数为197，则用197维向量表示。\u003c/li\u003e\n\u003cli\u003e例子：美国 \u003ccode\u003e[1, 0, 0, 0, …]\u003c/code\u003e， 中国 \u003ccode\u003e[0, 1, 0, 0, …]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e示例：表示一个人的特征\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征包含（年龄, 性别, 国籍，其中国籍197种）\u003c/li\u003e\n\u003cli\u003e维度总和：1维_age + 1维_gender + 197维_nationality = 199维。\u003c/li\u003e\n\u003cli\u003e例如 \u003ccode\u003e(28, 女, 中国)\u003c/code\u003e → \u003ccode\u003e[28, 0, 0, 1, 0, …, 0]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"三文本数据处理流程\"\u003e三、文本数据处理流程\u003c/h2\u003e\n\u003cp\u003e文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e分词 (Tokenization)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将文本拆分为一个个单词（Token）。\u003c/li\u003e\n\u003cli\u003e例子：\u003ccode\u003e\u0026quot;... to be or not to be...\u0026quot;\u003c/code\u003e → \u003ccode\u003e[\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统计词频 \u0026amp; 构建词典\u003c/strong\u003e\u003c/p\u003e","title":""},{"content":"Text to Sequence 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\n结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp-2/","summary":"\u003ch3 id=\"text-to-sequence\"\u003eText to Sequence\u003c/h3\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150322238\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150322238.png\"\u003e\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。\u003cimg alt=\"image-20250322150620890\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150620890.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结果\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150754528\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150754528.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322150854986\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322150854986.png\"\u003e\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151030119\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151030119.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151117125\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151117125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"./NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e","title":""},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"}]