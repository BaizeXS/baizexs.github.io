[{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„ åˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"../../../../Downloads/NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151511757\" loading=\"lazy\" src=\"../../../../Downloads/NLP-2.assets/image-20250322151511757.png\"\u003e\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\n$$ Y = ax + b $$\nä¸‰ã€æ–‡æœ¬æ•°æ®å¤„ç†æµç¨‹ æ–‡æœ¬åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ä½œä¸ºå¸¸è§çš„åŸå§‹è¾“å…¥ï¼Œéœ€è¦æ•°å€¼åŒ–æ‰èƒ½é€å…¥æ¨¡å‹ã€‚ä¸€èˆ¬æ­¥éª¤å¦‚ä¸‹ï¼š\nåˆ†è¯ (Tokenization)\nå°†æ–‡æœ¬æ‹†åˆ†ä¸ºä¸€ä¸ªä¸ªå•è¯ï¼ˆTokenï¼‰ã€‚ ä¾‹å­ï¼š\u0026quot;... to be or not to be...\u0026quot; â†’ [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]ã€‚ ç»Ÿè®¡è¯é¢‘ \u0026amp; æ„å»ºè¯å…¸\nè¯é¢‘ç»Ÿè®¡ ç”¨å“ˆå¸Œè¡¨/å­—å…¸è®°å½•æ¯ä¸ªå•è¯å‡ºç°æ¬¡æ•°ã€‚ éå†æ–‡æœ¬æ—¶ï¼Œè‹¥å•è¯å°šæœªå…¥è¡¨ï¼Œåˆ™æ–°å»ºè®°å½•ï¼›è‹¥å·²å­˜åœ¨ï¼Œåˆ™è®¡æ•°+1ã€‚ æ’åºä¸ç´¢å¼• æŒ‰è¯é¢‘é™åºæ’åºï¼Œè¯é¢‘æœ€é«˜çš„å•è¯æ’åœ¨å‰é¢ã€‚ ç´¢å¼•ä»1å¼€å§‹ï¼Œ0ç•™ç»™â€œæœªçŸ¥â€ã€‚ è¯å…¸è§„æ¨¡ï¼ˆvocabularyï¼‰å¯è§†éœ€æ±‚æˆªæ–­ï¼ˆä¾‹å¦‚åªä¿ç•™Top 10kï¼‰ã€‚ ä¸ºä»€ä¹ˆç§»é™¤ä½é¢‘è¯ ä½é¢‘è¯å¯èƒ½æ˜¯æ‹¼å†™é”™è¯¯ã€ç½•è§äººåç­‰ï¼Œå¯¹æ¨¡å‹å¸®åŠ©æœ‰é™ã€‚ å‡å°‘è¯å…¸è§„æ¨¡å¯é™ä½è¿ç®—å¼€é”€ã€åŠ å¿«æ¨¡å‹è®­ç»ƒä¸æ¨ç†é€Ÿåº¦ã€‚ ç‹¬çƒ­ç¼–ç  (One-Hot Encoding)\nå°†æ¯ä¸ªå•è¯æ˜ å°„ä¸ºè¯å…¸ä¸­çš„ç´¢å¼•åï¼ˆå¦‚ [2, 4, 1, 8, â€¦]ï¼‰ï¼Œå¯è¿›ä¸€æ­¥è½¬æ¢æˆOne-Hotå‘é‡ã€‚ One-Hotç»´åº¦å–å†³äºä¿ç•™ä¸‹æ¥çš„è¯å…¸å¤§å°ï¼ˆä¾‹å¦‚1ä¸‡ï¼‰ã€‚ æœªæ”¶å½•å•è¯å¯èƒ½ç›´æ¥å¿½ç•¥ï¼Œæˆ–ç»Ÿä¸€ç”¨ç´¢å¼•0ä»£æ›¿ã€‚ å››ã€æ ¸å¿ƒæ”¶è·ä¸åº”ç”¨ ç±»åˆ«ç‰¹å¾å¿…éœ€æ•°å€¼åŒ–ï¼š\nä½¿ç”¨æ•´æ•°1,2,3â€¦ä»…è¡¨ç¤ºç±»åˆ«â€œæ ‡è¯†â€ï¼Œå¹¶ä¸ä»£è¡¨å®ƒä»¬å¯è¿›è¡Œæ•°å€¼è¿ç®—ã€‚ å› æ­¤éœ€è¦One-Hotç­‰ç¼–ç æ–¹å¼ï¼Œé¿å…æ•°å€¼è¯¯è§£ã€‚ æ–‡æœ¬æ•°æ®é¢„å¤„ç†ï¼š\nåˆ†è¯ã€è¯é¢‘ç»Ÿè®¡ã€æ„å»ºè¯å…¸ã€ç´¢å¼•åŒ–ã€One-Hotæˆ–å…¶ä»–æ–¹å¼ï¼ˆå¦‚è¯åµŒå…¥ï¼‰æ˜¯å¸¸è§æµç¨‹ã€‚ æ­£ç¡®çš„æ–‡æœ¬é¢„å¤„ç†å¯æ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½å’Œæ•ˆç‡ã€‚ ä¿ç•™æœªçŸ¥ç´¢å¼•ï¼š\nä¿è¯å¯¹ç¼ºå¤±æ•°æ®æˆ–å­—å…¸å¤–æ–°è¯èƒ½æœ‰åŸºæœ¬çš„è¡¨ç¤ºï¼Œæå‡æ¨¡å‹å®¹é”™æ€§ã€‚ å®é™…ä½¿ç”¨ï¼š\nå¯¹äºå°è§„æ¨¡ç±»åˆ«å¯ç›´æ¥One-Hotç¼–ç ï¼›å¯¹äºè¯å…¸å¾ˆå¤§çš„åœºæ™¯ï¼Œé€šå¸¸ä¼šè¿›ä¸€æ­¥è€ƒè™‘è¯åµŒå…¥ï¼ˆWord Embeddingï¼‰æˆ–é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚ https://github.com/BaizeXS/baizexs.github.io/tree/main/content\nhttps://github.com/BaizeXS/baizexs.github.io/content/posts/NLP01.md\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„ åˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"../../../../Downloads/NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151511757\" loading=\"lazy\" src=\"../../../../Downloads/NLP-2.assets/image-20250322151511757.png\"\u003e\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion Through this study, we explored the fundamentals of data processing in machine learning including numerical, binary, and categorical features, as well as essential techniques such as One-Hot encoding and tokenization in text data processing. Understanding these concepts is crucial in building robust machine learning models that effectively handle various types of data.\nNext Steps Moving forward, consider diving deeper into:\nAdvanced feature engineering techniques to enhance model performance. Exploring different models that can utilize text data effectively, such as Recurrent Neural Networks (RNNs) or Transformers in Natural Language Processing. ","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nNext Steps As we move forward, we will focus on:\nText Processing Techniques: Learn methods for cleaning and normalizing text data, preparing it for modeling. Word Embedding: Investigate how words can be represented in vector space to capture their meanings, including various techniques such as Word2Vec and GloVe, which are vital for enhancing the performance of RNNs in NLP tasks. ","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nNext Steps As we move forward, we will focus on:\nText Processing Techniques: Learn methods for cleaning and normalizing text data, preparing it for modeling. Word Embedding: Investigate how words can be represented in vector space to capture their meanings, including various techniques such as Word2Vec and GloVe, which are vital for enhancing the performance of RNNs in NLP tasks. ","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nNext Steps As we move forward, we will focus on:\nText Processing Techniques: Learn methods for cleaning and normalizing text data, preparing it for modeling. Word Embedding: Investigate how words can be represented in vector space to capture their meanings, including various techniques such as Word2Vec and GloVe, which are vital for enhancing the performance of RNNs in NLP tasks. ","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nNext Steps Moving forward, we will delve into:\nText Processing Techniques: We will learn advanced methods for preprocessing text data, including techniques for cleaning, normalizing, and enriching the data to improve model performance. Word Embeddings: We will explore how to represent words in vector space, focusing on techniques like Word2Vec and GloVe that capture semantic relationships, enabling RNNs to understand context and meaning in language. ","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: Course Video Title.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: Course Video Title.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: RNNæ¨¡å‹ä¸NLPåº”ç”¨(1/9)ï¼šæ•°æ®å¤„ç†åŸºç¡€.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: RNNæ¨¡å‹ä¸NLPåº”ç”¨(1/9)ï¼šæ•°æ®å¤„ç†åŸºç¡€.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: RNNæ¨¡å‹ä¸NLPåº”ç”¨(1/9)ï¼šæ•°æ®å¤„ç†åŸºç¡€.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: RNNæ¨¡å‹ä¸NLPåº”ç”¨(1/9)ï¼šæ•°æ®å¤„ç†åŸºç¡€.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: RNNæ¨¡å‹ä¸NLPåº”ç”¨(1/9)ï¼šæ•°æ®å¤„ç†åŸºç¡€.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video:RNNæ¨¡å‹ä¸NLPåº”ç”¨(1/9)ï¼šæ•°æ®å¤„ç†åŸºç¡€.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video:RNNæ¨¡å‹ä¸NLPåº”ç”¨(1/9)ï¼šæ•°æ®å¤„ç†åŸºç¡€.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video:RNNæ¨¡å‹ä¸NLPåº”ç”¨(1/9)ï¼šæ•°æ®å¤„ç†åŸºç¡€.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding æ–‡æœ¬å¤„ç†\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\nå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\nä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\nStep4: å¯¹é½sequence\nè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\nè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\næ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\nç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\nå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\næˆ‘ä»¬çš„ä»»åŠ¡æ˜¯çœ‹è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼›å‚æ•°çŸ©é˜µæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å‡ºæ¥çš„ï¼Œæ‰€ä»¥å­¦å‡ºæ¥çš„è¯å‘é‡æ˜¯å¸¦æœ‰æ„Ÿæƒ…è‰²å½©çš„ï¼›å‡è®¾è¿™äº›è¯å‘é‡éƒ½æ˜¯äºŒç»´çš„ï¼Œåˆ™å¯ä»¥åœ¨å¹³é¢åæ ‡ç³»ä¸­æ ‡å‡ºä¸‹é¢è¯å‘é‡\nkerasæä¾›embeddingå±‚ï¼›ç”¨æˆ·å¯ä»¥æŒ‡å®švocabularyå¤§å°å’Œdçš„å¤§å°ï¼›ä»¥åŠæ¯ä¸€ä¸ªsequenceçš„é•¿åº¦ï¼›dæ˜¯æ ¹æ®ç®—æ³•é€‰å‡ºæ¥çš„\nåˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ–‡æœ¬å¤„ç†å’Œword embeddingã€‚æ¥ä¸‹æ¥å°±æ˜¯ç”¨Logistic Regression for Binary ClassificationåšäºŒåˆ†ç±»ï¼›\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenizationè®²ç©¶å¾ˆå¤šï¼Œä¾‹å¦‚æ˜¯å¦åº”è¯¥æŠŠå¤§å†™æ”¹æˆå°å†™å‘¢ï¼Ÿè¿›è¡Œtypo correctionç­‰ç­‰\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eå¯ä»¥é¦–å…ˆç»Ÿè®¡è¯é¢‘å»æ‰ä½é¢‘è¯ï¼Œç„¶åè®©æ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªæ­£æ•´æ•°ï¼›è¿™æ ·çš„è¯ï¼Œä¸€å¥è¯å°±å¯ä»¥ç”¨ä¸€ä¸ªæ­£æ•´æ•°åˆ—è¡¨è¡¨ç¤ºï¼Œè¿™ä¸ªåˆ—è¡¨å°±å«sequenceï¼›å¦‚æœå¿…è¦å°±è¿˜å¾—åšone-hot encodingã€‚ç»“æœ\u003c/p\u003e\n\u003cp\u003eä½†æ˜¯é•¿åº¦ä¸ç»Ÿä¸€\u003c/p\u003e\n\u003cp\u003eStep4: å¯¹é½sequence\u003c/p\u003e\n\u003cp\u003eè§£å†³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ï¼šå¯ä»¥å›ºå®šé•¿åº¦ä¸ºwï¼ŒåŠ å…¥ä¸€ä¸ªåºåˆ—é•¿åº¦å¤ªé•¿ï¼Œå°±ç æ‰å‰é¢çš„è¯ï¼Œåªä¿ç•™æœ€åwä¸ªè¯ï¼›å¦‚æœä¸€ä¸ªåºåˆ—å¤ªçŸ­ï¼Œå°±åšzero paddingï¼Œç”¨0å¡«å……\u003c/p\u003e\n\u003cp\u003eè¿™æ ·ä¸€æ¥ï¼Œæ‰€æœ‰çš„åºåˆ—é•¿åº¦å°±ç»Ÿä¸€äº†\u003c/p\u003e\n\u003cp\u003eæ–‡æœ¬å¤„ç†å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æ˜¯word embeddingï¼ŒæŠŠå•è¯è¡¨ç¤ºä¸ºä¸€ä¸ªä½ç»´å‘é‡ï¼›\u003c/p\u003e\n\u003cp\u003eç„¶è€Œè¿™æ ·åšçš„è¯ï¼Œå¦‚æœæ˜¯10000ä¸ªå•è¯ï¼Œç»´åº¦å°±æ˜¯10000ç»´ï¼Œç»´åº¦å¤ªé«˜äº†ï¼›å› æ­¤è¦åšword embeddingï¼›\u003c/p\u003e\n\u003cp\u003eå…·ä½“åšæ³•å°±æ˜¯æŠŠone-hotå‘é‡e_iå’Œå‚æ•°çŸ©é˜µpç›¸ä¹˜ï¼›çŸ©é˜µPè½¬ç½®çš„å¤§å°æ˜¯d x vï¼›dæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼›væ˜¯å­—å…¸é‡Œè¯æ±‡çš„æ•°é‡ï¼›ç»“æœä¸ºx_iï¼›x_iå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å¦‚æœone-hotå‘é‡çš„ç¬¬ä¸‰è¡Œä¸º1ï¼›é‚£ä¹ˆx_iå°±æ˜¯Pè½¬ç½®çŸ©é˜µçš„ç¬¬ä¸‰åˆ—ï¼ˆå…¶ä½™åˆ—éƒ½ä¸º0ï¼‰ï¼›æ‰€ä»¥Pè½¬ç½®æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›æ‰€ä»¥çŸ©é˜µPæœ¬èº«çš„æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯å‘é‡ï¼›å…¶è¡Œæ•°ä¸ºvï¼›å³vocabularyï¼Œè¯æ±‡é‡ï¼›æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå•è¯ï¼›çŸ©é˜µçš„åˆ—æ•°ä¸ºdï¼›dæ˜¯ç”¨æˆ·å†³å®šçš„ï¼›dçš„å¤§å°ä¼šå†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼›åº”è¯¥ç”¨xxxæ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒå¥½çš„dï¼›\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, â€¦, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = â€œâ€¦ to be or not to beâ€¦â€ We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), itâ€™s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video:RNNæ¨¡å‹ä¸NLPåº”ç”¨(1/9)ï¼šæ•°æ®å¤„ç†åŸºç¡€.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"ğŸ‘‹ About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\nğŸ“š Education ğŸ‡­ğŸ‡° The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. ğŸ‡¨ğŸ‡³ Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). ğŸ† Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸ’¼ Project Experience ğŸ” Online Transaction Fraud Detection 09/2024 â€“ 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. ğŸ¤– Multi-Model Intelligent Text Keyword Extraction System 08/2023 â€“ 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. ğŸ“ Cambridge University Machine Learning Summer Project 07/2023 â€“ 08/2023 Group project, supervised by Prof. Pietro LiÃ² Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. ğŸ‘ï¸ Deep Learning and Computer Vision Semester Project 09/2022 â€“ 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. ğŸ“ Internship âš›ï¸ Beijing Academy of Quantum Information Science 08/2023 â€“ 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. ğŸ… Competition Experience ğŸ¥‰ China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 ğŸ›¡ï¸ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. ğŸ¥‰ \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 ğŸŒ³ Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. ğŸ“‹ Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 ğŸ“« Contact ğŸ“§ Email: zongsi.xu@outlook.com ğŸ‘” LinkedIn: Zongsi (Tristan) Xu ğŸ’» GitHub: BaizeXS ğŸ“ Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"}]