[{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"../../../../Downloads/NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151511757\" loading=\"lazy\" src=\"../../../../Downloads/NLP-2.assets/image-20250322151511757.png\"\u003e\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\n$$ Y = ax + b $$\n三、文本数据处理流程 文本在自然语言处理（NLP）中作为常见的原始输入，需要数值化才能送入模型。一般步骤如下：\n分词 (Tokenization)\n将文本拆分为一个个单词（Token）。 例子：\u0026quot;... to be or not to be...\u0026quot; → [\u0026quot;...\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;not\u0026quot;, \u0026quot;to\u0026quot;, \u0026quot;be\u0026quot;, \u0026quot;...\u0026quot;]。 统计词频 \u0026amp; 构建词典\n词频统计 用哈希表/字典记录每个单词出现次数。 遍历文本时，若单词尚未入表，则新建记录；若已存在，则计数+1。 排序与索引 按词频降序排序，词频最高的单词排在前面。 索引从1开始，0留给“未知”。 词典规模（vocabulary）可视需求截断（例如只保留Top 10k）。 为什么移除低频词 低频词可能是拼写错误、罕见人名等，对模型帮助有限。 减少词典规模可降低运算开销、加快模型训练与推理速度。 独热编码 (One-Hot Encoding)\n将每个单词映射为词典中的索引后（如 [2, 4, 1, 8, …]），可进一步转换成One-Hot向量。 One-Hot维度取决于保留下来的词典大小（例如1万）。 未收录单词可能直接忽略，或统一用索引0代替。 四、核心收获与应用 类别特征必需数值化：\n使用整数1,2,3…仅表示类别“标识”，并不代表它们可进行数值运算。 因此需要One-Hot等编码方式，避免数值误解。 文本数据预处理：\n分词、词频统计、构建词典、索引化、One-Hot或其他方式（如词嵌入）是常见流程。 正确的文本预处理可显著影响模型性能和效率。 保留未知索引：\n保证对缺失数据或字典外新词能有基本的表示，提升模型容错性。 实际使用：\n对于小规模类别可直接One-Hot编码；对于词典很大的场景，通常会进一步考虑词嵌入（Word Embedding）或预训练语言模型。 https://github.com/BaizeXS/baizexs.github.io/tree/main/content\nhttps://github.com/BaizeXS/baizexs.github.io/content/posts/NLP01.md\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的 到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151252837\" loading=\"lazy\" src=\"../../../../Downloads/NLP-2.assets/image-20250322151252837.png\"\u003e\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20250322151511757\" loading=\"lazy\" src=\"../../../../Downloads/NLP-2.assets/image-20250322151511757.png\"\u003e\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion Through this study, we explored the fundamentals of data processing in machine learning including numerical, binary, and categorical features, as well as essential techniques such as One-Hot encoding and tokenization in text data processing. Understanding these concepts is crucial in building robust machine learning models that effectively handle various types of data.\nNext Steps Moving forward, consider diving deeper into:\nAdvanced feature engineering techniques to enhance model performance. Exploring different models that can utilize text data effectively, such as Recurrent Neural Networks (RNNs) or Transformers in Natural Language Processing. ","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nNext Steps As we move forward, we will focus on:\nText Processing Techniques: Learn methods for cleaning and normalizing text data, preparing it for modeling. Word Embedding: Investigate how words can be represented in vector space to capture their meanings, including various techniques such as Word2Vec and GloVe, which are vital for enhancing the performance of RNNs in NLP tasks. ","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nNext Steps As we move forward, we will focus on:\nText Processing Techniques: Learn methods for cleaning and normalizing text data, preparing it for modeling. Word Embedding: Investigate how words can be represented in vector space to capture their meanings, including various techniques such as Word2Vec and GloVe, which are vital for enhancing the performance of RNNs in NLP tasks. ","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nNext Steps As we move forward, we will focus on:\nText Processing Techniques: Learn methods for cleaning and normalizing text data, preparing it for modeling. Word Embedding: Investigate how words can be represented in vector space to capture their meanings, including various techniques such as Word2Vec and GloVe, which are vital for enhancing the performance of RNNs in NLP tasks. ","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nNext Steps Moving forward, we will delve into:\nText Processing Techniques: We will learn advanced methods for preprocessing text data, including techniques for cleaning, normalizing, and enriching the data to improve model performance. Word Embeddings: We will explore how to represent words in vector space, focusing on techniques like Word2Vec and GloVe that capture semantic relationships, enabling RNNs to understand context and meaning in language. ","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: Course Video Title.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: Course Video Title.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: RNN模型与NLP应用(1/9)：数据处理基础.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: RNN模型与NLP应用(1/9)：数据处理基础.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: RNN模型与NLP应用(1/9)：数据处理基础.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: RNN模型与NLP应用(1/9)：数据处理基础.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video: RNN模型与NLP应用(1/9)：数据处理基础.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video:RNN模型与NLP应用(1/9)：数据处理基础.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video:RNN模型与NLP应用(1/9)：数据处理基础.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video:RNN模型与NLP应用(1/9)：数据处理基础.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"},{"content":"","permalink":"http://localhost:1313/posts/nlp09/","summary":"","title":"NLP09"},{"content":"","permalink":"http://localhost:1313/posts/nlp08/","summary":"","title":"NLP08"},{"content":"","permalink":"http://localhost:1313/posts/nlp07/","summary":"","title":"NLP07"},{"content":"","permalink":"http://localhost:1313/posts/nlp06/","summary":"","title":"NLP06"},{"content":"","permalink":"http://localhost:1313/posts/nlp05/","summary":"","title":"RNN and NLP Study Notes 05"},{"content":"","permalink":"http://localhost:1313/posts/nlp04/","summary":"","title":"RNN and NLP Study Notes 04"},{"content":"","permalink":"http://localhost:1313/posts/nlp03/","summary":"","title":"RNN and NLP Study Notes 03"},{"content":"Text Processing and Word Embedding 文本处理\nStep1: Tokenization\nTokenization breaks a piece of text down into a list of tokens.\nHere, a token is a word. (A token can be a character in some applications.)\nTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\nStep2: Build Dictionary\nUse a dictionary (hash table) to count word frequencies.\nThe dictionary maps word to index.\n可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\n但是长度不统一\nStep4: 对齐sequence\n解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\n这样一来，所有的序列长度就统一了\n文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\n然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\n具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\n我们的任务是看评论是正面的还是负面的；参数矩阵是从训练数据中学习出来的，所以学出来的词向量是带有感情色彩的；假设这些词向量都是二维的，则可以在平面坐标系中标出下面词向量\nkeras提供embedding层；用户可以指定vocabulary大小和d的大小；以及每一个sequence的长度；d是根据算法选出来的\n到这一步，我们已经完成了文本处理和word embedding。接下来就是用Logistic Regression for Binary Classification做二分类；\n","permalink":"http://localhost:1313/posts/nlp02/","summary":"\u003ch2 id=\"text-processing-and-word-embedding\"\u003eText Processing and Word Embedding\u003c/h2\u003e\n\u003cp\u003e文本处理\u003c/p\u003e\n\u003cp\u003eStep1: Tokenization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTokenization breaks a piece of text down into a list of tokens.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHere, a token is a word. (A token can be a character in some applications.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTokenization讲究很多，例如是否应该把大写改成小写呢？进行typo correction等等\u003c/p\u003e\n\u003cp\u003eStep2: Build Dictionary\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eUse a dictionary (hash table) to count word frequencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe dictionary maps word to index.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以首先统计词频去掉低频词，然后让每个词对应一个正整数；这样的话，一句话就可以用一个正整数列表表示，这个列表就叫sequence；如果必要就还得做one-hot encoding。结果\u003c/p\u003e\n\u003cp\u003e但是长度不统一\u003c/p\u003e\n\u003cp\u003eStep4: 对齐sequence\u003c/p\u003e\n\u003cp\u003e解决方案是这样的：可以固定长度为w，加入一个序列长度太长，就砍掉前面的词，只保留最后w个词；如果一个序列太短，就做zero padding，用0填充\u003c/p\u003e\n\u003cp\u003e这样一来，所有的序列长度就统一了\u003c/p\u003e\n\u003cp\u003e文本处理完成了，接下来是word embedding，把单词表示为一个低维向量；\u003c/p\u003e\n\u003cp\u003e然而这样做的话，如果是10000个单词，维度就是10000维，维度太高了；因此要做word embedding；\u003c/p\u003e\n\u003cp\u003e具体做法就是把one-hot向量e_i和参数矩阵p相乘；矩阵P转置的大小是d x v；d是词向量的维度；v是字典里词汇的数量；结果为x_i；x_i就是一个词向量；如果one-hot向量的第三行为1；那么x_i就是P转置矩阵的第三列（其余列都为0）；所以P转置每一列都是一个词向量；所以矩阵P本身的每一行就是一个词向量；其行数为v；即vocabulary，词汇量；每一行对应一个单词；矩阵的列数为d；d是用户决定的；d的大小会决定机器学习模型的表现；应该用xxx来选择一个比较好的d；\u003c/p\u003e","title":"RNN and NLP Study Notes 02"},{"content":"Data Processing Basics In machine learning, data types can be classified into several forms, including numeric features, categorical features, and binary features. The table below provides concrete examples to better understand these data types:\nAge Gender Nationality 35 Male US 31 Male China 29 Female India 27 Male US Numeric Features Numeric features refer to data that possess additive properties and can be compared in magnitude. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\nBinary Features Binary features typically represent scenarios with only two possible values, such as gender. Gender can be represented by 0 (female) and 1 (male).\nCategorical Features Categorical features are used to represent different categories, such as nationality. Due to the diversity of countries, numeric vectors are needed for representation. A dictionary can be created to map nationalities to corresponding indices, such as US=1, China=2, India=3, and so on.\nIssue with Categorical Features Representing categorical features using integer values may lead to incorrect numerical relationships. For instance, the expression US + China = India is ridiculous.\nSolution: One-Hot Encoding One-Hot encoding is a method used to represent categorical variables as binary vectors, effectively mitigating the issues associated with ordinal relationships. For example, nationalities can be encoded as follows:\nThe U.S. is represented as [1, 0, 0, 0, ..., 0]. China is represented as [0, 1, 0, 0, ..., 0]. India is represented as [0, 0, 1, 0, …, 0]. In this way, if a person holds both U.S. and Indian citizenship, their encoded representation could be a combination of those two vectors: [1, 0, 1, 0, ..., 0]. This indicates that they possess both nationalities.\nOne-Hot Encoding In machine learning, One-Hot encoding is a crucial technique used to represent categorical variables as binary vectors. This method helps to preserve the integrity of the data while avoiding misleading numerical relationships.\nWhy Use One-Hot Encoding Avoiding Pseudo-Numeric Relationships: One-Hot encoding prevents false numeric relationships that can arise from integer encoding. By mapping categories to vectors, this method ensures that the model does not make incorrect inferences based on arbitrary numerical values. Complete Information Retention: This encoding method retains all possible category information by assigning a value of 1 at the position corresponding to each category, with 0s elsewhere. This structure simplifies the learning process for the model, allowing it to effectively distinguish between different categories. Steps of One-Hot Encoding One-Hot encoding is typically performed in two main steps:\nEstablishing Mapping: From Category to Index\nAssign a unique index to each category, typically starting from 1. Reserve index 0 to represent \u0026ldquo;unknown\u0026rdquo; or \u0026ldquo;missing,\u0026rdquo; which corresponds to a vector of all 0s. This enhances the model\u0026rsquo;s tolerance for missing data. Converting to One-Hot Vector\nEach category is transformed into a One-Hot vector, where a value of 1 is placed in the index corresponding to that category and 0s are placed elsewhere.\nFor example, if there are a total of 197 categories (such as countries), each category would be represented by a 197-dimensional vector:\nThe United States can be represented as [1, 0, 0, 0, ...], indicating its position in the vector.\nChina can be represented as [0, 1, 0, 0, ...], indicating its position as well.\nThese steps ensure that each category can be uniquely identified without introducing misleading numeric relationships.\nExample: Representing a Person\u0026rsquo;s Features When analyzing a person\u0026rsquo;s characteristics, several key aspects come into play, including age, gender, and nationality. For our purposes, let\u0026rsquo;s assume there are 197 nationalities.\nTo calculate the total number of feature dimensions, we can break it down as follows:\nAge: 1 dimension Gender: 1 dimension Nationality: 197 dimensions This gives us a total of 199 dimensions to represent a person\u0026rsquo;s features.\nFor example, consider a person with the attributes (28, Female, China). The features can be represented by the following One-Hot vector:\n1 [28, 0, 0, 1, 0, ..., 0] In this vector:\nThe age retains its original value of 28. Gender is represented by 0 (indicating female). The One-Hot encoding for nationality indicates a 1 at the position corresponding to China, ensuring that the categorical integrity is preserved. This structured approach allows us to effectively model and analyze individual characteristics in a machine learning context.\nProcessing Text Data Processing text data effectively is essential for applications in natural language processing (NLP) and machine learning. This section outlines the key steps involved in processing text data.\nStep 1: Tokenization (Text to Words) The first step in processing text data is tokenization, where a string of text is broken down into a list of individual words. For example, given the text:\n1 S = “… to be or not to be…” We break this string into a list of words:\n1 L = [..., to, be, or, not, to, be, ...] Step 2: Count Word Frequencies Once we have the list of words, the next step is to count the frequency of each word. This can be done using a dictionary (or hash table) that keeps track of each word and its corresponding frequency.\nInitially, the dictionary is empty. For each word ($w$): If the $w$ is not in the dictionary, add $w$ with a frequency of 1. If $w$ is in the dictionary, increment its frequency counter. For example, after processing some text, the dictionary might look like this:\nKey(word) Value(frequency) a 219 to 398 hamlet 5 be 131 not 499 prince 12 kill 31 Step 3: Limit Vocabulary Size If the vocabulary (the number of unique words) is too large (e.g., over 10,000), it’s advisable to keep only the most frequent words. This is necessary because:\nInfrequent words often add little meaningful information, like typos or rare names. A larger vocabulary leads to higher-dimensional one-hot vectors, which increases computation time and the number of parameters in the word embedding layer. Step 4: One-Hot Encoding In the final step, we map each word to its index using one-hot encoding. For instance, if the words are:\n1 Words: [to, be, or, not, to, be] the corresponding indices might be:\n1 Indices: [2, 4, 8, 1, 2, 4] Each word can then be represented as a one-hot vector. If a word cannot be found in the dictionary (such as a typo), it can either be ignored or encoded as 0.\nConclusion In this section, we explored essential concepts of data processing, which are foundational for effectively applying Recurrent Neural Networks (RNNs) in Natural Language Processing (NLP). We examined different data types, emphasizing numerical, binary, and categorical features. Additionally, we highlighted preprocessing techniques like One-Hot encoding and tokenization, which are crucial for transforming textual data into a format suitable for RNNs and other machine learning models.\nAdditional Resources For a deeper understanding of these concepts, you can refer to the following course video:RNN模型与NLP应用(1/9)：数据处理基础.\n","permalink":"http://localhost:1313/posts/nlp01/","summary":"\u003ch2 id=\"data-processing-basics\"\u003eData Processing Basics\u003c/h2\u003e\n\u003cp\u003eIn machine learning, data types can be classified into several forms, including \u003cstrong\u003enumeric features\u003c/strong\u003e, \u003cstrong\u003ecategorical features\u003c/strong\u003e, and \u003cstrong\u003ebinary features\u003c/strong\u003e. The table below provides concrete examples to better understand these data types:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAge\u003c/th\u003e\n          \u003cth\u003eGender\u003c/th\u003e\n          \u003cth\u003eNationality\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e35\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e31\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eChina\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e29\u003c/td\u003e\n          \u003ctd\u003eFemale\u003c/td\u003e\n          \u003ctd\u003eIndia\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e27\u003c/td\u003e\n          \u003ctd\u003eMale\u003c/td\u003e\n          \u003ctd\u003eUS\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"numeric-features\"\u003eNumeric Features\u003c/h3\u003e\n\u003cp\u003eNumeric features refer to data that possess \u003cstrong\u003eadditive properties\u003c/strong\u003e and \u003cstrong\u003ecan be compared in magnitude\u003c/strong\u003e. For example, a person\u0026rsquo;s age serves as a numeric feature, where 35 is greater than 31.\u003c/p\u003e","title":"RNN and NLP Study Notes 01"},{"content":"👋 About me Hello! I\u0026rsquo;m Xu Zongsi, a passionate technology enthusiast currently pursuing my Master of Science in ECIC at The University of Hong Kong. With a solid background in Software Engineering from the Beijing Institute of Technology, I specialize in artificial intelligence areas like machine learning, deep learning, natural language processing, computer vision, and reinforcement learning.\nInspired by OpenAI\u0026rsquo;s five-step plan towards superintelligence, I am excited about AGI and have developed a keen interest in LLM agents, MLLM, and knowledge graphs. I see great potential in these fields and aspire to contribute to innovative solutions.\n📚 Education 🇭🇰 The University of Hong Kong 09/2024 - Expected 11/2025 School of Computing and Data Science Master of Science in Electronic Commerce \u0026amp; Internet Computing Relevant Courses: Machine learning for business and e-commerce, Knowledge graphs, Financial fraud analytics, Internet infrastructure technologies, Website engineering, Blockchain and cryptocurrency, etc. 🇨🇳 Beijing Institute of Technology 09/2020 - 06/2024 School of Computer Science \u0026amp; Technology Bachelor of Engineering in Software Engineering, last two years GPA: 3.7/4 Relevant Courses: Reinforcement Learning, Deep Learning and Computer Vision, Artificial Intelligence and Robot Ethics, Almost Human: Mind, Brain and Artificial Intelligence, Mathematical Analysis for Engineering, Linear Algebra, Probability and Mathematical Statistics, etc. Skills: Python, Java, common data structures, design patterns, machine learning basics and algorithms, Linux, PyTorch, and some development tools (git, conda, etc.). 🏆 Awards \u0026amp; Honors Beijing Institute of Technology Undergraduate Scholarship (Top 20% of Grade) 06/2023 3rd Prize of China International \u0026#34;Internet \u0026#43;\u0026#34; College Student\u0026#39;s Innovation Competition 08/2022 Bronze Prize of \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 💼 Project Experience 🔍 Online Transaction Fraud Detection 09/2024 – 01/2025 Independent project, supervised by Dr. Vivien Chan Focused on addressing financial fraud in e-commerce to enhance the accuracy of transaction anomaly detection, while independently managing the entire case study that included data preprocessing, feature engineering, model building, evaluation, and results analysis. Effectively resolved class imbalance issues using SMOTE and ROSE techniques. Built various models including Logistic Regression, Random Forest, XGBoost, and LightGBM, ultimately selecting the LightGBM model to achieve a recall rate of 95.45% and an AUC of 99.84%. Wrote efficient and maintainable code, enhancing model training and data processing efficiency through parallel computing and logging. 🤖 Multi-Model Intelligent Text Keyword Extraction System 08/2023 – 09/2023 Group project, supervised by Associate Prof. Xiaolin Zhao Designed and implemented an intelligent text summary system that automatically extracts keywords from article titles and abstracts, enhancing the understanding and summarization of text information. Participated in the planning of the technical route and built a hybrid model combining extractive and generative approaches, utilizing advanced natural language processing techniques including KeyBERT, Sentence Transformer, BERT, and T5 models. Responsible for data preprocessing and tokenization, as well as the end-to-end implementation and training of the T5 model, optimizing the model architecture and hyperparameters, achieving satisfactory results after only 3 epochs of training. Model evaluation results showed that the Rouge-1, Rouge-2, and Rouge-L metrics reached 0.6345, 0.5038, and 0.5282, respectively, in the third epoch, validating the model\u0026rsquo;s effectiveness in the summarization task. 🎓 Cambridge University Machine Learning Summer Project 07/2023 – 08/2023 Group project, supervised by Prof. Pietro Liò Participated in the Cambridge University deep learning summer project, focusing on deep reinforcement learning; explored multi-agent learning algorithms across different environments. Reproduced the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm in the Simple Adversary environment and studied the effect of agent numbers on convergence speed and stability, finding optimal stability at 21 agents. Designed and implemented comparative experiments inspired by the Rainbow algorithm, assessing target network strategies versus the Double Deep Q-Network (Double DQN). The results demonstrated that Double DQN significantly enhanced convergence speed, optimizing the overall learning process. Co-authored a group project paper on agent numbers and strategies effectiveness in multi-agent systems, achieving a score of 71% in the summer program (the highest in my group) and a \u0026ldquo;First Class\u0026rdquo; classification in the UK grading system. 👁️ Deep Learning and Computer Vision Semester Project 09/2022 – 12/2022 Independent project, supervised by Associate Prof. Guangyu Gao Designed and implemented a waste classifier using the ResNet model to improve classification accuracy and reduce resource waste. The project also enhanced understanding of transfer learning techniques with ResNet. Collected a dataset of 16 waste categories and preprocessed the images for uniformity and standardization, incorporating data augmentation techniques to enhance the dataset. Conducted experiments using ResNet34 and ResNet50 models through transfer learning, accelerating network convergence by leveraging pretrained weights. During training, techniques such as Batch Normalization and Dropout were applied to mitigate the risk of overfitting. The final model achieved an accuracy of 90% on the training set, while the performance on the test set was 61%, indicating a significant overfitting phenomenon during the training process. 🎓 Internship ⚛️ Beijing Academy of Quantum Information Science 08/2023 – 01/2024 Research Intern at Quantum Computation Department Proposed a quantum convolution kernel design based on variational quantum circuits and developed a universal quantum convolution layer, successfully integrating it into classical CNN architectures such as VGG, GoogLeNet, and ResNet. Developed and evaluated various hybrid models, including a simple model named HQCCNN-2, which achieved 84.45% accuracy on the FashionMNIST dataset, surpassing the traditional CNN\u0026rsquo;s 80.72%. This demonstrates the effectiveness of the quantum convolution layer in improving model performance. Tested on a real quantum computer further validated the effectiveness of the hybrid model, demonstrating the viable integration of quantum computing with deep learning, and highlighting the potential for leveraging quantum properties to enhance model performance. 🏅 Competition Experience 🥉 China International \u0026#34;Internet \u0026#43;\u0026#34; College Students\u0026#39; Innovation Competition 08/2022 🛡️ Zhongke Ushield: Deep Learning-based Face Privacy Protection Authentication System Responsible for constructing a localized facial feature extraction engine based on MobileFaceNet, ensuring feature extraction is completed on end devices (such as mobile phones and cameras), guaranteeing that raw data does not leave the domain and is not stored, thus enhancing user privacy protection while improving recognition efficiency. 🥉 \u0026#34;Challenge Cup\u0026#34; Capital University Entrepreneurship Competition 05/2022 🌳 Smart Orchard Responsible for the development of the fruit detection, ripeness prediction, and pest identification system, using deep convolutional neural network (CNN) technology for intelligent monitoring of crop growth conditions. Applied advanced object detection algorithms such as Faster R-CNN and YOLO to achieve high precision in apple detection and ripeness assessment, with an accuracy rate exceeding 90%, significantly enhancing the intelligence level of orchard management. Built and trained a dedicated dataset containing various pest samples, optimizing pest detection accuracy through data preprocessing (such as image normalization and resizing) and data augmentation techniques (like rotation, flipping, and noise addition), enabling the real-time monitoring system to accurately identify over 80% of pests, providing effective scientific maintenance recommendations to help farmers reduce crop losses. 📋 Extracurricular Activities Vice President, Cyber Security Club, Beijing Institute of Technology 09/2021-09/2022 Backbone, Media Center of the Student Union of Beijing Institute of Technology 10/2020-06/2021 📫 Contact 📧 Email: zongsi.xu@outlook.com 👔 LinkedIn: Zongsi (Tristan) Xu 💻 GitHub: BaizeXS 📍 Location: Hong Kong SAR, China ","permalink":"http://localhost:1313/resume/","summary":"Personal resume of Zongsi (Tristan) Xu","title":"Zongsi (Tristan) Xu"}]