---
title: "RNN and NLP Study Notes 04"
date: "2025-03-24T11:41:35+08:00"
tags: [""]
author: "Tristan"
draft: false
description: ""
# canonicalURL: "https://canonical.url/to/page"
---

## Long Short Term Memory (LSTM)

### LSTM Model

Long Short-Term Memory networks (LSTM) are a specialized type of recurrent neural network (RNN) designed to effectively handle dependencies in long sequence data. Traditional RNNs often face the problem of forgetting when processing long time sequences, making it difficult to retain important information. LSTM addresses this issue by introducing a "cell state" mechanism that allows information to flow directly across time steps, helping the network better capture long-term dependencies.

![image-20250326144600920](/images/image-20250326144600920.png)

The structure of LSTM is more complex than that of a standard simple RNN. Each of its repeating modules consists of multiple interacting neural network layers rather than just a single layer. Specifically, LSTM uses four parameter matrices to control the forgetting, updating, and output of information, which enhances its capability in modeling sequence data. The design of LSTM's input gate, forget gate, and output gate ensures that it can actively select which information should be retained or updated, allowing it to excel in processing complex sequence tasks. This flexibility makes LSTM widely used in fields such as natural language processing and speech recognition, helping to solve many practical problems.

### Step-by-Step LSTM Walk Through

The core of LSTM lies in its "cell state," which is a horizontal line that runs through the entire network. The cell state can be thought of as a conveyor belt that flows directly through the network while engaging in only a small amount of linear interaction. This allows information to flow easily and remain unchanged along this conveyor belt.

![image-20250326145155175](/images/image-20250326145155175.png)

The "gates" in LSTM are mechanisms that can selectively allow information to pass through. Each gate consists of a sigmoid neural network layer and a pointwise multiplication operation. The sigmoid layer outputs a value between 0 and 1, determining how much of each piece of information should be allowed to pass. A value of 0 means "no information is allowed to pass," while a value of 1 means "all information is allowed to pass.‚Äù

![image-20250326145807368](/images/image-20250326145807368.png)

LSTM has three such gates, responsible for protecting and controlling the cell state, ensuring that information can flow and update effectively within the network.

#### Forget Gate

The forget gate $f_t$ in Long Short-Term Memory (LSTM) networks is responsible for determining which information from the cell state should be discarded, consisting mainly of a sigmoid function and elementwise multiplication.

![image-20250326151710194](/images/image-20250326151710194.png)

First, the forget gate concatenates the previous hidden state vector $h$ with the current input vector $x$ to form a new vector. Then, it applies the sigmoid function to compress each element of this concatenated vector to a value between 0 and 1, determining the extent to which each piece of information should be retained.

![image-20250326151436309](/images/image-20250326151436309.png)

Subsequently, each element of the output vector $f_t$ generated by the forget gate is multiplied elementwise with the corresponding element of the cell state $c$. This means that if any element of $f_t$ is 0, the corresponding element of $c$ will be completely forgotten, resulting in an output of 0; conversely, if an element of $f_t$ is 1, the corresponding element of $c$ can fully pass through, maintaining its value. Therefore, the design of the forget gate allows the LSTM to flexibly choose which information to retain and which to discard.

![image-20250326151451949](/images/image-20250326151451949.png)

Mathematically, the calculation of the forget gate is represented as:
$$
f(t) = W_f \cdot \sigma([h_{t-1},x_t] + b_f)
$$
Here, $W_f$ is the weight matrix of the forget gate, $\sigma$ is the sigmoid activation function, $[h_{t-1}, x_t]$ represents the concatenation of the previous hidden state $h_{t-1}$ and the current input $x_t$, and $b_f$ is the bias term. In this way, the forget gate can effectively control the flow of information, ensuring the effective learning and stable operation of the LSTM network.

#### Input Gate

The input gate $i_t$ in Long Short-Term Memory (LSTM) networks decides which values from the conveyor belt will be updated. It relies on the previous hidden state $h_{t-1}$ and the current input $x_t$. The first step of the input gate is similar to that of the forget gate, where it concatenates $h_{t-1}$ and $x_t$, then applies the sigmoid function to obtain the output $i_t$, which ranges between (0, 1). 

![image-20250326161507245](/images/image-20250326161507245.png)

Mathematically, the calculation of the input gate is represented as:
$$
i(t) = W_i \cdot \sigma([h_{t-1}, x_t] + b_i)
$$
Here, $W_i$ is the weight matrix for the input gate, $\sigma$ is the sigmoid activation function, and $b_i$ is the bias term.

#### Candidate Memory Cell

In addition to the input gate, we need to compute the new candidate value $\tilde{c}_t$, which will be added to the conveyor belt. This computation is similar to that of $i_t$ but uses the $\tanh$ function instead of the sigmoid function. Thus, the elements of $\tilde{c}_t$ range between $(-1, 1)$.

![image-20250326161749809](/images/image-20250326161749809.png)

Mathematically, the calculation of the candidate memory cell is represented as:
$$
\tilde{c}(t) = W_c \cdot \tanh([h_{t-1}, x_t] + b_c)
$$
Here, $W_c$ is the weight matrix for the candidate memory cell, $\tanh$ is the hyperbolic tangent activation function, and $b_c$ is the bias term.

#### Updating the Cell State

With the known values of the previous cell state $C_{t-1}$, the output of the forget gate $f_t$, the output of the input gate $i_t$, and the candidate value $\tilde{c}_t$, we can now proceed to update the cell state $C$.

The forget gate $f_t$ is multiplied elementwise with the old cell state $C_{t-1}$ to determine which elements to forget. If any element of $f_t$ is 0, the corresponding element of $C_{t-1}$ will be completely forgotten.

![image-20250326162118762](/images/image-20250326162118762.png)

Next, after selectively forgetting some elements in the cell state, we need to add new information to the conveyor belt. This is done by performing elementwise multiplication between the input gate output $i_t$ and the candidate value $\tilde{c}_t$, followed by adding these two results together to produce the new cell state $C_t$.

![image-20250326162147608](/images/image-20250326162147608.png)

Mathematically, the update of the cell state is represented as:
$$
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{c}_t
$$
Where $\odot$ denotes elementwise multiplication.

This completes one update cycle of the conveyor belt. After updating the cell state $C$, we need to compute the LSTM's output $h_t$. First, we calculate the output gate $o_t$, which follows a similar procedure to that of the input gate and the forget gate.

#### Output Gate

The output gate $o_t$ in Long Short-Term Memory (LSTM) networks decides what information flows from the conveyor belt $C_t$ to the hidden state $h_t$. It takes into account the previous hidden state $h_{t-1}$ and the current input $x_t$.

The first step involves concatenating $h_{t-1}$ and $x_t$, after which the concatenated vector is processed through a sigmoid activation function to produce $o_t$. The output $o_t$ ranges between $(0, 1)$, indicating the extent to which information should be kept from the cell state.

![image-20250326163741544](/images/image-20250326163741544.png)

Mathematically, the calculation of the output gate is represented as:
$$
o(t) = W_o \cdot \sigma([h_{t-1}, x_t] + b_o)
$$
Here, $W_o$ is the weight matrix for the output gate, $\sigma$ is the sigmoid activation function, and $b_o$ is the bias term.

#### Calculating the Output State

To compute the output state $h_t$, we first apply the hyperbolic tangent function ($\tanh$) to the current cell state $C_t$. This compresses each element to a range of $(-1, 1)$. Then, we multiply the result elementwise by the output gate $o_t$.

![image-20250326164043978](/images/image-20250326164043978.png)

Mathematically, this is expressed as:
$$
h_t = o_t \odot \tanh(C_t)
$$
Here, $o_t$ provides the gating mechanism that controls which parts of $C_t$ are passed to $h_t$ through elementwise multiplication $\odot$.

#### State Representation

The output state $h_t$ serves as the final output of the LSTM. It has two copies: one forwarded to the next time step and the other used as the current output. By the time step $t$, all input vectors $x$ processed by the LSTM contribute to the information in $h_t$, enabling the network to effectively utilize historical context for its predictions.

This structure ensures that the LSTM can maintain relevant information across time steps while controlling what is outputted in each cycle.

### Understanding $C_t$ and $h_t$ in LSTM

In Long Short-Term Memory (LSTM) networks, $C_t$ and $h_t$ are two critical state variables, each serving distinct purposes:

1. $C_t$ (Cell State):
   - Represents **long-term memory** within the LSTM structure.
   - Used to carry information that helps the network remember long-term dependencies when processing sequential data.
   - It is updated by **retaining important information** and **forgetting unnecessary** details, typically controlled by the forget gate and input gate.
   - $C_t$ is a single vector that is continuously passed along through the time steps.
2. $h_t$ (Hidden State):
   - Represents **short-term memory** within the LSTM structure.
   - Captures the output of the current time step, which can be used for predicting the next time step's input.
   - $h_t$ is derived from the cell state after passing through a nonlinear transformation (usually the $\tanh$ function) and is sent to the network's output layer.
   - It can be viewed as a summary of the information at the current time step, serving as the output of the network at that moment.

In summary, $C_t$ serves to store long-term memories, allowing the network to track information more meticulously, while $h_t$ is a short-term representation of that state, emphasizing the output at the current time step. Together, these states enable LSTMs to effectively process time series data and learn long-range dependencies.

### Model Parameters of LSTM

When discussing the parameters of LSTM, attention should be paid to the dimensions of each parameter matrix. Specifically, there are a total of **four parameter matrices** in LSTM. The number of rows in each parameter matrix corresponds to the dimension of the hidden state $shape(h)$, while the number of columns is the sum of the hidden state dimension and the input dimension $shape(h) + shape(x)$. Therefore, the total number of parameters in these matrices can be represented as:

- #parameter matrices: 4
- #rows in each matrix: $shape(h)$
- #columns in each matrix: $shape(h) + shape(x)$
- Total #parameters (not counting intercept): $4 \times shape(h) \times [shape(h) + shape(x)]$

This means that as the input scale increases, the number of parameters in LSTM will also grow accordingly, thereby influencing the model's complexity and training requirements.

### Summary

- LSTM uses a ‚Äúconveyor belt‚Äù to get longer memory than SimpleRNN.

- Each of the following blocks has a parameter matrix:

  - Forget gate.
  - Input gate.
  - Candidate Memory Cell.
  - Output gate.

- Number of parameters:
  $$
  4 \times shape(h) \times [shape(h) + shape(x)]
  $$

- Dropout can also be used in LSTM.

### Reference

For a deeper understanding of these concepts, you can refer to the following course video:[RNNÊ®°Âûã‰∏éNLPÂ∫îÁî®(4/9)ÔºöLSTMÊ®°Âûã](https://youtu.be/vTouAvxlphc?si=qu9kilJ2P_gcZXsL).
